{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/openai-python/blob/main/examples/embeddings/Get_embeddings.ipynb\n",
    "# %pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Note\n",
    "Output embedding model contains only one row per  document. All documents with missing PMID or abstracts or very long abstracts (>2000 tokens) are discarded.\n",
    "\n",
    "# Input\n",
    "- `data/pubmed/abstracts.csv.gz` contains raw un-preprocessed texts. Duplicate articles will be discarded to speed up model fitting.\n",
    "\n",
    "# Outputs\n",
    "\n",
    "- `models/abstracts_gpt3curie.npz` for the embedding weights one row per document.\n",
    "- `models/abstracts_pmids_gpt3curie.csv` includes PMIDs for the rows of the above matrix. THis can be use to connect weights to the actual PubMed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from python.cogtext.datasets.pubmed import PubMedDataLoader\n",
    "from python.cogtext.similarity_matrix import get_similarity_matrix\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Embedding 385704 abstracts (pmids in models/abstracts_pmids_gpt3curie.npz)\n"
     ]
    }
   ],
   "source": [
    "# load and prep pubmed document\n",
    "pubmed = PubMedDataLoader(preprocessed=False,\n",
    "                          drop_low_occurred_labels=True).load()\n",
    "pubmed = pubmed.query('pmid.notna() and abstract.notna()')\n",
    "pubmed['abstract'] = pubmed['abstract'].apply(lambda x: x.replace('\\n', ' '))\n",
    "pubmed = pubmed.drop_duplicates(subset=['pmid'])\n",
    "\n",
    "# remove a very long document that prevented GPT-3 to encode all the other documents\n",
    "very_long_doc_index = pubmed.query('abstract.str.len() == abstract.str.len().max()').index\n",
    "pubmed.drop(index=very_long_doc_index, inplace=True)\n",
    "\n",
    "# or a slower RegEx approach\n",
    "# abstract_tokens = pubmed['abstract'].apply(lambda x: len(re.split('\\W+|\\s+', x)))\n",
    "# pubmed = pubmed[abstract_tokens < 2000]\n",
    "\n",
    "pubmed[['pmid']].to_csv('models/abstracts_pmids_gpt3curie.npz')\n",
    "\n",
    "print(f'* Embedding {pubmed.shape[0]} abstracts (pmids in models/abstracts_pmids_gpt3curie.npz)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 documents already embedded.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = Path('models/abstracts_gpt3curie.npz')\n",
    "\n",
    "N_PREFITTED_DOCUMENTS = 0\n",
    "\n",
    "if OUTPUT_PATH.exists():\n",
    "  available_embeddings = np.load(OUTPUT_PATH)['arr_0']\n",
    "  N_PREFITTED_DOCUMENTS = available_embeddings.shape[0]\n",
    "  \n",
    "print(f'* {N_PREFITTED_DOCUMENTS} documents already embedded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|â–Œ         | 220/3858 [07:08<4:44:10,  4.69s/batch]"
     ]
    }
   ],
   "source": [
    "batch_size = 100\n",
    "embeddings_dim = 2048  # GPT-3 Curie\n",
    "gpt3_model_id = 'ada'\n",
    "model = openai.Engine(id=f'{gpt3_model_id}-similarity')\n",
    "embeddings = []\n",
    "\n",
    "# @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def gpt3_embed(texts: list[str]):\n",
    "  try:\n",
    "    Z = model.embeddings(input=texts)#['data']['embedding']\n",
    "    Z = [z['embedding'] for z in Z['data']]\n",
    "    Z = np.array(Z)\n",
    "  except Exception as e :\n",
    "    print('GPT-3 failed! Filling the batch with zeros.', e)\n",
    "    Z = np.zeros((len(texts), embeddings_dim))\n",
    "  return Z\n",
    "\n",
    "for i in tqdm(range(N_PREFITTED_DOCUMENTS, len(pubmed), batch_size), unit='batch'):\n",
    "  batch = pubmed[i:i+batch_size]['abstract'].tolist()\n",
    "  batch_embeddings = gpt3_embed(batch)\n",
    "  embeddings.append(batch_embeddings)\n",
    "\n",
    "embeddings = np.vstack(embeddings)\n",
    "\n",
    "if N_PREFITTED_DOCUMENTS > 0:\n",
    "  embeddings = np.vstack([available_embeddings, embeddings])\n",
    "\n",
    "np.savez(f'models/abstracts_gpt3{gpt3_model_id}.npz', embeddings)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d4c55ad0dd25f9ca95e4d49a929aa3f71bfb37020ae570a9996c3e164818202"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('py3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
