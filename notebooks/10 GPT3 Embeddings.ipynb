{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/openai-python/blob/main/examples/embeddings/Get_embeddings.ipynb\n",
    "%pip install openai -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "Output embedding model contains only one row per  document. All documents with missing PMID or abstracts or very long abstracts are discarded as GPT-3 requires documents to be 2047 tokens or less.\n",
    "\n",
    "# Input\n",
    "- `data/pubmed/abstracts.csv.gz` contains raw un-preprocessed texts. Duplicate articles will be discarded to speed up model fitting.\n",
    "\n",
    "# Outputs\n",
    "\n",
    "- `models/abstracts_gpt3ada.npz` for the embedding weights; one row per document.\n",
    "- `models/gpt3/abstracts_pmids_gpt3ada.csv` includes PMIDs for the rows of the above matrix. This can be use to connect weights to the actual PubMed datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import openai\n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from python.cogtext.datasets.pubmed import PubMedDataLoader\n",
    "from python.cogtext.similarity_matrix import get_similarity_matrix\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt3_model_id = 'ada'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 382855 abstracts (pmids in models/gpt3/abstracts_ada_pmids.npz)\n"
     ]
    }
   ],
   "source": [
    "# load and prep pubmed document\n",
    "pubmed = PubMedDataLoader(preprocessed=False,\n",
    "                          drop_low_occurred_labels=True).load()\n",
    "pubmed = pubmed.query('pmid.notna() and abstract.notna() and title.notna()')\n",
    "pubmed['abstract'] = pubmed['abstract'].apply(lambda x: x.replace('\\n', ' '))\n",
    "pubmed = pubmed.drop_duplicates(subset=['pmid'])\n",
    "\n",
    "\n",
    "#### REMOVE VERY LONG ABSTRACTS; GPT-3 is limited to 2047 tokens per document\n",
    "\n",
    "# remove a very long document that prevented GPT-3 to encode all the other documents\n",
    "very_long_docs = pubmed['abstract'].str.len().sort_values()[:-11:-1]\n",
    "pubmed.drop(index=very_long_docs.index, inplace=True)\n",
    "\n",
    "# and just a heuristic to avoid GPT-3 error when encoding documents\n",
    "pubmed = pubmed.query('abstract.str.len() < 3000')\n",
    "\n",
    "# or a slower RegEx approach\n",
    "# abstract_tokens = pubmed['abstract'].apply(lambda x: len(re.split('\\W+|\\s+', x)))\n",
    "# pubmed = pubmed[abstract_tokens < 2000]\n",
    "\n",
    "pubmed[['pmid']].to_csv(f'models/gpt3/abstracts_{gpt3_model_id}_pmids.npz')\n",
    "\n",
    "print(f'* {pubmed.shape[0]} abstracts (pmids in models/gpt3/abstracts_{gpt3_model_id}_pmids.npz)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* 0 documents already embedded.\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_PATH = Path(f'models/gpt3/abstracts_{gpt3_model_id}.npz')\n",
    "\n",
    "N_PREFITTED_DOCUMENTS = 0\n",
    "\n",
    "if OUTPUT_PATH.exists():\n",
    "  available_embeddings = np.load(OUTPUT_PATH)['arr_0']\n",
    "  N_PREFITTED_DOCUMENTS = available_embeddings.shape[0]\n",
    "  \n",
    "print(f'* {N_PREFITTED_DOCUMENTS} documents already embedded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3829/3829 [2:35:23<00:00,  2.43s/batch]  \n"
     ]
    }
   ],
   "source": [
    "gpt3_embeddings_dims = {\n",
    "  'ada': 1024,\n",
    "  'babbage': 2048,\n",
    "  'curie': 4096,\n",
    "  'davinci': 12288\n",
    "}\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "model = openai.Engine(id=f'{gpt3_model_id}-similarity')\n",
    "\n",
    "# @retry(wait=wait_random_exponential(min=1, max=20), stop=stop_after_attempt(6))\n",
    "def gpt3_embed(texts: list[str]):\n",
    "  try:\n",
    "    Z = model.embeddings(input=texts)#['data']['embedding']\n",
    "    Z = [z['embedding'] for z in Z['data']]\n",
    "    Z = np.array(Z)\n",
    "  except Exception as e:\n",
    "    print('GPT-3 failed! Filling the batch with zeros.', e)\n",
    "    Z_dim = gpt3_embeddings_dims[gpt3_model_id]\n",
    "    Z = np.zeros((len(texts), Z_dim))\n",
    "  return Z\n",
    "\n",
    "for i in tqdm(range(N_PREFITTED_DOCUMENTS, len(pubmed), batch_size), unit='batch'):\n",
    "  batch = pubmed[i:i+batch_size]['abstract'].tolist()\n",
    "  batch_embeddings = gpt3_embed(batch)\n",
    "  \n",
    "  # cache\n",
    "  np.savez(\n",
    "    f'models/gpt3/abstracts_{gpt3_model_id}_b{(int(i/batch_size)+1):05d}.npz',\n",
    "    batch_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code will convert the GPT-3 embedding to NetCDF and store it in the `models/abstracts_gpt3ada.nc` file. Use XArray to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "pmids = pd.read_csv('models/gpt3/abstracts_gpt3ada_pmids.csv', index_col=0)\n",
    "pmids.index.name = 'original_index'\n",
    "\n",
    "embeddings = np.load('models/abstracts_gpt3ada.npz')['arr_0']\n",
    "\n",
    "# DEBUG embeddings.shape, pmids.shape\n",
    "\n",
    "# create the dataset\n",
    "dataset = xr.Dataset({\n",
    "  'embeddings': (['pmid', 'embedding'], embeddings)\n",
    "},\n",
    "coords={\n",
    "  'original_index': pmids.index.values,\n",
    "  'pmid': pmids['pmid'].values\n",
    "})\n",
    "\n",
    "# store\n",
    "dataset.to_netcdf('models/abstracts_gpt3ada.nc',)\n",
    "                  # encoding={'embeddings':{'zlib':True, 'complevel':9}})\n",
    "\n",
    "# DEBUG report dimensionalities\n",
    "# dataset['pmid'].shape, dataset['original_index'].shape, dataset['embeddings'].shape\n",
    "\n",
    "# open the dataset and print it for validation\n",
    "xr.open_dataset('models/abstracts_gpt3ada.nc')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d4c55ad0dd25f9ca95e4d49a929aa3f71bfb37020ae570a9996c3e164818202"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('py3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
