{"cells":[{"cell_type":"code","execution_count":null,"source":["%reload_ext autoreload\n","%autoreload 2\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, explained_variance_score, r2_score\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader, random_split\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from python.cogtext.utils import select_relevant_pubmed_articles\n","from python.cogtext.coappearance_matrix import generate_coappearance_matrix_fast\n","\n","sns.set()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":228,"source":["# parameters\n","DEV_MODE = True\n","INPUT_FILE = 'data/pubmed_abstracts.csv.gz'"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":null,"source":["# prepare data\n","\n","PUBMED = (pd.read_csv(INPUT_FILE)\n","            .pipe(select_relevant_pubmed_articles)\n","            .dropna(subset=['abstract']))\n","\n","# only corpora with # of articles < DEV_MAX_CORPUS_SIZE\n","# subcats_cnt = PUBMED['subcategory'].value_counts()\n","# small_subcats = subcats_cnt[subcats_cnt < DEV_MAX_CORPUS_SIZE].index.to_list()\n","# PUBMED = PUBMED.query('subcategory in @small_subcats',).copy()\n","\n","# DROP tasks/constructs with less than 5 articles (1/test + 1/valid + 4/train = 6)\n","valid_subcats = PUBMED['subcategory'].value_counts()[lambda cnt: cnt > 5].index.to_list()\n","PUBMED = PUBMED.query('subcategory in @valid_subcats')\n","\n","# train/test split (80% train 20% test)\n","PUBMED_train, PUBMED_test = train_test_split(\n","    PUBMED,\n","    test_size=0.2,\n","    stratify=PUBMED['subcategory'],\n","    random_state=0)\n","\n","n_constructs = PUBMED.groupby('category')['subcategory'].nunique()['CognitiveConstruct']\n","n_tasks = PUBMED.groupby('category')['subcategory'].nunique()['CognitiveTask']"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":261,"source":["class MFNet(nn.Module):\n","  def __init__(self, n_tasks, n_constructs, n_embeddings):\n","    super(MFNet, self).__init__()\n","    self.task_embeddings = nn.Embedding(n_tasks, n_embeddings)\n","    self.construct_embeddings = nn.Embedding(n_constructs, n_embeddings)\n","    self.task_biases = torch.nn.Embedding(n_tasks, 1)\n","    self.construct_biases = torch.nn.Embedding(n_constructs, 1)\n","    self.decoder = nn.Linear(n_embeddings, 1)\n","\n","  def forward(self, construct, task):\n","    B = self.task_biases(task) + self.construct_biases(construct)\n","    M = self.task_embeddings(task)\n","    C = self.construct_embeddings(construct)\n","    y = B + M @ C.T\n","    # y = self.decoder(H)\n","    return y\n","\n","  def fit(self,\n","      X, y,\n","      train_split_size=.8,\n","      n_epochs=1000,\n","      batch_size=100,\n","      logger: SummaryWriter=SummaryWriter()):\n","\n","    assert 0. < train_split_size < 1.0\n","\n","    n_samples = X.shape[0]\n","\n","    train_size = int(n_samples * train_split_size)\n","    test_size = n_samples - train_size\n","\n","    dataset = TensorDataset(X, y)\n","    train_subset, test_subset = random_split(dataset, lengths=(train_size,test_size))\n","\n","    X_test, y_test = dataset[test_subset.indices]\n","\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(self.parameters(), lr=.001)\n","\n","    logger.add_graph(model, X)\n","\n","    for epoch in range(n_epochs):\n","\n","      # train model\n","      model.train()\n","      for X_batch, y_batch in DataLoader(train_subset, batch_size=batch_size):\n","        model.zero_grad()\n","\n","        # X_train = torch.tensor(X).type(torch.int)\n","        # y_train = torch.tensor(y).type(torch.float)\n","        # y_pred = self(X_train[:, 0], X_train[:, 1])\n","\n","        y_pred = model(X_batch)\n","        loss = criterion(y_batch, y_pred)\n","        logger.add_scalar('loss/train', loss.detach(), epoch)\n","        loss.backward()\n","        optimizer.step()\n","\n","      # eval mode\n","      model.eval()\n","      with torch.no_grad():\n","        y_pred = model(X_test)\n","        loss = criterion(y_test, y_pred)\n","        logger.add_scalar('loss/test', loss.detach(), epoch)\n","        \n","        ev = explained_variance_score(y_test, y_pred)\n","        logger.add_scalar('explained_variance/test', ev, epoch)\n","\n","    return self"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":262,"source":["# TODO\n","cols = ['construct','task']\n","\n","# create CO_APPEARANCE\n","CO_APPEARANCE = PUBMED.pipe(generate_coappearance_matrix_fast, probability=True, group_categories=True)\n","X = np.vstack([CO_APPEARANCE[c].astype('category').cat.codes for c in cols]).T\n","y = CO_APPEARANCE[['probability']].values\n","\n","# TODO drop 0 probabilities from X\n","# TODO n_embeddings should be a hyper parameter (use Ax to optimize)\n","\n","model = MFNet(n_tasks, n_constructs, 7)\n","model.fit(X, y)"],"outputs":[{"output_type":"stream","name":"stderr","text":["/Users/morteza/miniconda3/envs/py3/lib/python3.9/site-packages/torch/nn/modules/loss.py:528: UserWarning: Using a target size (torch.Size([])) that is different to the input size (torch.Size([5893, 5893])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n","  return F.mse_loss(input, target, reduction=self.reduction)\n"]},{"output_type":"stream","name":"stdout","text":["epoch=0, loss=111.417\n","epoch=1, loss=111.341\n","epoch=2, loss=111.265\n","epoch=3, loss=111.190\n","epoch=4, loss=111.114\n","epoch=5, loss=111.039\n","epoch=6, loss=110.964\n","epoch=7, loss=110.888\n","epoch=8, loss=110.813\n","epoch=9, loss=110.738\n","epoch=10, loss=110.663\n","epoch=11, loss=110.589\n","epoch=12, loss=110.514\n","epoch=13, loss=110.439\n","epoch=14, loss=110.365\n","epoch=15, loss=110.291\n","epoch=16, loss=110.216\n","epoch=17, loss=110.142\n","epoch=18, loss=110.068\n","epoch=19, loss=109.994\n","epoch=20, loss=109.920\n","epoch=21, loss=109.847\n","epoch=22, loss=109.773\n","epoch=23, loss=109.699\n","epoch=24, loss=109.626\n","epoch=25, loss=109.552\n","epoch=26, loss=109.479\n","epoch=27, loss=109.405\n","epoch=28, loss=109.332\n","epoch=29, loss=109.259\n","epoch=30, loss=109.186\n","epoch=31, loss=109.113\n","epoch=32, loss=109.040\n","epoch=33, loss=108.967\n","epoch=34, loss=108.894\n","epoch=35, loss=108.821\n","epoch=36, loss=108.748\n","epoch=37, loss=108.675\n","epoch=38, loss=108.602\n","epoch=39, loss=108.530\n","epoch=40, loss=108.457\n","epoch=41, loss=108.384\n","epoch=42, loss=108.311\n","epoch=43, loss=108.239\n","epoch=44, loss=108.166\n","epoch=45, loss=108.093\n","epoch=46, loss=108.020\n","epoch=47, loss=107.948\n","epoch=48, loss=107.875\n","epoch=49, loss=107.802\n","epoch=50, loss=107.729\n","epoch=51, loss=107.657\n","epoch=52, loss=107.584\n","epoch=53, loss=107.511\n","epoch=54, loss=107.438\n","epoch=55, loss=107.365\n","epoch=56, loss=107.292\n","epoch=57, loss=107.219\n","epoch=58, loss=107.146\n","epoch=59, loss=107.072\n","epoch=60, loss=106.999\n","epoch=61, loss=106.926\n","epoch=62, loss=106.852\n","epoch=63, loss=106.779\n","epoch=64, loss=106.705\n","epoch=65, loss=106.631\n","epoch=66, loss=106.558\n","epoch=67, loss=106.484\n","epoch=68, loss=106.410\n","epoch=69, loss=106.336\n","epoch=70, loss=106.262\n","epoch=71, loss=106.187\n","epoch=72, loss=106.113\n","epoch=73, loss=106.038\n","epoch=74, loss=105.964\n","epoch=75, loss=105.889\n","epoch=76, loss=105.814\n","epoch=77, loss=105.739\n","epoch=78, loss=105.664\n","epoch=79, loss=105.589\n","epoch=80, loss=105.513\n","epoch=81, loss=105.437\n","epoch=82, loss=105.362\n","epoch=83, loss=105.286\n","epoch=84, loss=105.210\n","epoch=85, loss=105.133\n","epoch=86, loss=105.057\n","epoch=87, loss=104.980\n","epoch=88, loss=104.903\n","epoch=89, loss=104.826\n","epoch=90, loss=104.749\n","epoch=91, loss=104.672\n","epoch=92, loss=104.594\n","epoch=93, loss=104.516\n","epoch=94, loss=104.438\n","epoch=95, loss=104.360\n","epoch=96, loss=104.282\n","epoch=97, loss=104.203\n","epoch=98, loss=104.124\n","epoch=99, loss=104.045\n","epoch=100, loss=103.966\n","epoch=101, loss=103.887\n","epoch=102, loss=103.807\n","epoch=103, loss=103.727\n","epoch=104, loss=103.647\n","epoch=105, loss=103.566\n","epoch=106, loss=103.485\n","epoch=107, loss=103.404\n","epoch=108, loss=103.323\n","epoch=109, loss=103.242\n","epoch=110, loss=103.160\n","epoch=111, loss=103.078\n","epoch=112, loss=102.996\n","epoch=113, loss=102.913\n","epoch=114, loss=102.830\n","epoch=115, loss=102.747\n","epoch=116, loss=102.664\n","epoch=117, loss=102.580\n","epoch=118, loss=102.496\n","epoch=119, loss=102.412\n","epoch=120, loss=102.327\n","epoch=121, loss=102.242\n","epoch=122, loss=102.157\n","epoch=123, loss=102.072\n","epoch=124, loss=101.986\n","epoch=125, loss=101.900\n","epoch=126, loss=101.813\n","epoch=127, loss=101.727\n","epoch=128, loss=101.640\n","epoch=129, loss=101.552\n","epoch=130, loss=101.465\n","epoch=131, loss=101.377\n","epoch=132, loss=101.288\n","epoch=133, loss=101.200\n","epoch=134, loss=101.111\n","epoch=135, loss=101.021\n","epoch=136, loss=100.932\n","epoch=137, loss=100.842\n","epoch=138, loss=100.751\n","epoch=139, loss=100.660\n","epoch=140, loss=100.569\n","epoch=141, loss=100.478\n","epoch=142, loss=100.386\n","epoch=143, loss=100.294\n","epoch=144, loss=100.202\n","epoch=145, loss=100.109\n","epoch=146, loss=100.015\n","epoch=147, loss=99.922\n","epoch=148, loss=99.828\n","epoch=149, loss=99.734\n","epoch=150, loss=99.639\n","epoch=151, loss=99.544\n","epoch=152, loss=99.448\n","epoch=153, loss=99.352\n","epoch=154, loss=99.256\n","epoch=155, loss=99.160\n","epoch=156, loss=99.063\n","epoch=157, loss=98.965\n","epoch=158, loss=98.867\n","epoch=159, loss=98.769\n","epoch=160, loss=98.671\n","epoch=161, loss=98.572\n","epoch=162, loss=98.472\n","epoch=163, loss=98.373\n","epoch=164, loss=98.273\n","epoch=165, loss=98.172\n","epoch=166, loss=98.071\n","epoch=167, loss=97.970\n","epoch=168, loss=97.868\n","epoch=169, loss=97.766\n","epoch=170, loss=97.663\n","epoch=171, loss=97.560\n","epoch=172, loss=97.457\n","epoch=173, loss=97.353\n","epoch=174, loss=97.249\n","epoch=175, loss=97.144\n","epoch=176, loss=97.039\n","epoch=177, loss=96.934\n","epoch=178, loss=96.828\n","epoch=179, loss=96.721\n","epoch=180, loss=96.615\n","epoch=181, loss=96.508\n","epoch=182, loss=96.400\n","epoch=183, loss=96.292\n","epoch=184, loss=96.184\n","epoch=185, loss=96.075\n","epoch=186, loss=95.965\n","epoch=187, loss=95.856\n","epoch=188, loss=95.746\n","epoch=189, loss=95.635\n","epoch=190, loss=95.524\n","epoch=191, loss=95.413\n","epoch=192, loss=95.301\n","epoch=193, loss=95.189\n","epoch=194, loss=95.076\n","epoch=195, loss=94.963\n","epoch=196, loss=94.849\n","epoch=197, loss=94.735\n","epoch=198, loss=94.621\n","epoch=199, loss=94.506\n","epoch=200, loss=94.391\n","epoch=201, loss=94.275\n","epoch=202, loss=94.159\n","epoch=203, loss=94.043\n","epoch=204, loss=93.926\n","epoch=205, loss=93.808\n","epoch=206, loss=93.690\n","epoch=207, loss=93.572\n","epoch=208, loss=93.453\n","epoch=209, loss=93.334\n","epoch=210, loss=93.215\n","epoch=211, loss=93.095\n","epoch=212, loss=92.974\n","epoch=213, loss=92.853\n","epoch=214, loss=92.732\n","epoch=215, loss=92.610\n","epoch=216, loss=92.488\n","epoch=217, loss=92.365\n","epoch=218, loss=92.242\n","epoch=219, loss=92.119\n","epoch=220, loss=91.995\n","epoch=221, loss=91.871\n","epoch=222, loss=91.746\n","epoch=223, loss=91.621\n","epoch=224, loss=91.495\n","epoch=225, loss=91.369\n","epoch=226, loss=91.243\n","epoch=227, loss=91.116\n","epoch=228, loss=90.989\n","epoch=229, loss=90.861\n","epoch=230, loss=90.733\n","epoch=231, loss=90.604\n","epoch=232, loss=90.476\n","epoch=233, loss=90.346\n","epoch=234, loss=90.216\n","epoch=235, loss=90.086\n","epoch=236, loss=89.956\n","epoch=237, loss=89.825\n","epoch=238, loss=89.693\n","epoch=239, loss=89.561\n","epoch=240, loss=89.429\n","epoch=241, loss=89.296\n","epoch=242, loss=89.163\n","epoch=243, loss=89.030\n","epoch=244, loss=88.896\n","epoch=245, loss=88.762\n","epoch=246, loss=88.627\n","epoch=247, loss=88.492\n","epoch=248, loss=88.356\n","epoch=249, loss=88.220\n","epoch=250, loss=88.084\n","epoch=251, loss=87.947\n","epoch=252, loss=87.810\n","epoch=253, loss=87.673\n","epoch=254, loss=87.535\n","epoch=255, loss=87.397\n","epoch=256, loss=87.258\n","epoch=257, loss=87.119\n","epoch=258, loss=86.980\n","epoch=259, loss=86.840\n","epoch=260, loss=86.700\n","epoch=261, loss=86.559\n","epoch=262, loss=86.418\n","epoch=263, loss=86.277\n","epoch=264, loss=86.135\n","epoch=265, loss=85.993\n","epoch=266, loss=85.851\n","epoch=267, loss=85.708\n","epoch=268, loss=85.565\n","epoch=269, loss=85.422\n","epoch=270, loss=85.278\n","epoch=271, loss=85.134\n","epoch=272, loss=84.989\n","epoch=273, loss=84.844\n","epoch=274, loss=84.699\n","epoch=275, loss=84.553\n","epoch=276, loss=84.407\n","epoch=277, loss=84.261\n","epoch=278, loss=84.114\n","epoch=279, loss=83.968\n","epoch=280, loss=83.820\n","epoch=281, loss=83.673\n","epoch=282, loss=83.525\n","epoch=283, loss=83.376\n","epoch=284, loss=83.228\n","epoch=285, loss=83.079\n","epoch=286, loss=82.929\n","epoch=287, loss=82.780\n","epoch=288, loss=82.630\n","epoch=289, loss=82.480\n","epoch=290, loss=82.329\n","epoch=291, loss=82.178\n","epoch=292, loss=82.027\n","epoch=293, loss=81.875\n","epoch=294, loss=81.724\n","epoch=295, loss=81.572\n","epoch=296, loss=81.419\n","epoch=297, loss=81.267\n","epoch=298, loss=81.114\n","epoch=299, loss=80.960\n","epoch=300, loss=80.807\n","epoch=301, loss=80.653\n","epoch=302, loss=80.499\n","epoch=303, loss=80.344\n","epoch=304, loss=80.190\n","epoch=305, loss=80.035\n","epoch=306, loss=79.880\n","epoch=307, loss=79.724\n","epoch=308, loss=79.568\n","epoch=309, loss=79.412\n","epoch=310, loss=79.256\n","epoch=311, loss=79.099\n","epoch=312, loss=78.943\n","epoch=313, loss=78.786\n","epoch=314, loss=78.628\n","epoch=315, loss=78.471\n","epoch=316, loss=78.313\n","epoch=317, loss=78.155\n","epoch=318, loss=77.997\n","epoch=319, loss=77.838\n","epoch=320, loss=77.680\n","epoch=321, loss=77.521\n","epoch=322, loss=77.361\n","epoch=323, loss=77.202\n","epoch=324, loss=77.043\n","epoch=325, loss=76.883\n","epoch=326, loss=76.723\n","epoch=327, loss=76.562\n","epoch=328, loss=76.402\n","epoch=329, loss=76.241\n","epoch=330, loss=76.080\n","epoch=331, loss=75.919\n","epoch=332, loss=75.758\n","epoch=333, loss=75.597\n","epoch=334, loss=75.435\n","epoch=335, loss=75.273\n","epoch=336, loss=75.111\n","epoch=337, loss=74.949\n","epoch=338, loss=74.787\n","epoch=339, loss=74.624\n","epoch=340, loss=74.461\n","epoch=341, loss=74.299\n","epoch=342, loss=74.136\n","epoch=343, loss=73.972\n","epoch=344, loss=73.809\n","epoch=345, loss=73.646\n","epoch=346, loss=73.482\n","epoch=347, loss=73.318\n","epoch=348, loss=73.154\n","epoch=349, loss=72.990\n","epoch=350, loss=72.826\n","epoch=351, loss=72.661\n","epoch=352, loss=72.497\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/var/folders/3_/gmvd1nkx285133z5yh3chz2c0000gp/T/ipykernel_13726/4209512678.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOAPPEARANCES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'probability'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mMFNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_constructs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/var/folders/3_/gmvd1nkx285133z5yh3chz2c0000gp/T/ipykernel_13726/4127810710.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, n_epochs)\u001b[0m\n\u001b[1;32m     29\u001b[0m       \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m       \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m       \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m       \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/py3/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/miniconda3/envs/py3/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"orig_nbformat":4,"kernelspec":{"name":"python3","display_name":"Python 3.9.4 64-bit"},"interpreter":{"hash":"5ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84"}}}