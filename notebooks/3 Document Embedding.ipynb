{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Embedding\n",
    "\n",
    "This notebook uses [`jina-embeddings-v2-base-en` embedding model](https://huggingface.co/jinaai/jina-embeddings-v2-base-en) to transform abstract texts to a 768-dimensional embedding space.\n",
    "\n",
    "> **Note from our 2021 study:** In 2021, we used GPT-3 Embedding API to generate text similarity embeddings from the input text. Since the API was limited to pieces with less than 2048 token, we used heuristics to remove long abstracts with more than 2048 tokens. Note that, starting January 2022, GPT-3 Embedding API is NOT free anymore. You probably need to pay for it to run this notebook. Cached results from 2021 are available on HuggingFace.\n",
    "\n",
    "## Input\n",
    "- `data/pubmed/abstracts_2023.csv.gz` contains raw un-preprocessed texts collected from PubMed. To speed things up, duplicate documents will be queried only once. Documents with identical PMID are considered as duplicate.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- `models/embeddings/jina-embeddings-v2-base-en.safetensors`, in SafeTesnsors format, contains the PMIDs and corresponding embedding weights; one key per document.\n",
    "\n",
    "### Requirements\n",
    "\n",
    "You may need a valid HuggingFace access token to run this notebook. You can get one from [here](https://huggingface.co/settings/tokens).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "from IPython.display import display\n",
    "\n",
    "import polars as pl\n",
    "import torch\n",
    "from safetensors.torch import save_file, load_file\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from src.cogtext.datasets.pubmed import PubMedDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "HF_MODEL_NAME = 'jinaai/jina-embeddings-v2-small-en'\n",
    "MAX_TOKENS = 1024\n",
    "BATCH_SIZE = 64  # number of abstracts to embed at once\n",
    "\n",
    "OUTPUT_FILE = f'data/embeddings/abstracts2023_{HF_MODEL_NAME.split(\"/\")[1]}.safetensors'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare and cleanup the input data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique documents: 464765\n"
     ]
    }
   ],
   "source": [
    "data = PubMedDataset(year=2023).load()\n",
    "data = data.unique('pmid')\n",
    "\n",
    "print(f'Number of unique documents: {len(data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenizing 12223 candidate articles (n_words > 400)...\n",
      "Discarded 511 articles where n_tokens > 1023.\n"
     ]
    }
   ],
   "source": [
    "# discard abstracts where n_tokens > MAX_TOKENS\n",
    "\n",
    "candidate_abstracts = data.lazy().with_columns(\n",
    "    data.select(pl.col('abstract')\n",
    "                  .map_elements(lambda x: x.count(' '))\n",
    "                  .alias('abstract_len'))\n",
    ").filter(pl.col('abstract_len').gt(400)).sort('abstract_len', descending=True).collect()\n",
    "\n",
    "print(f'tokenizing {len(candidate_abstracts)} candidate articles (n_words > 400)...')\n",
    "model = SentenceTransformer(HF_MODEL_NAME, device='cuda').eval()\n",
    "\n",
    "def get_n_tokens(texts: pl.Series, tokenizer=model[0].tokenizer):\n",
    "    \"\"\"Helper function to count number of tokens in a batch of texts.\"\"\"\n",
    "    o = tokenizer(texts.to_list(), return_attention_mask=False, return_token_type_ids=False)\n",
    "    n_tokens = [len(tokens) for tokens in o['input_ids']]\n",
    "    return pl.Series(n_tokens)\n",
    "\n",
    "# count tokens for each candidate abstract\n",
    "candidate_abstracts = candidate_abstracts.with_columns(\n",
    "    candidate_abstracts.select(pl.col('abstract').map_batches(get_n_tokens).alias('n_tokens')))\n",
    "\n",
    "# find abstracts where n_tokens >= MAX_TOKENS\n",
    "pmids_to_discard = candidate_abstracts.filter(pl.col('n_tokens').ge(MAX_TOKENS))['pmid'].to_list()\n",
    "\n",
    "# filter long abstracts\n",
    "data = data.lazy().filter(~pl.col('pmid').is_in(pmids_to_discard)).collect()\n",
    "\n",
    "print(f'Discarded {len(pmids_to_discard)} articles where n_tokens > 1023.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 464254 existing embeddings.\n"
     ]
    }
   ],
   "source": [
    "# pipeline params\n",
    "\n",
    "tmp_dir = Path('tmp/embeddings') / HF_MODEL_NAME.split(\"/\")[1]  # e.g., tmp/embeddings/jina*\n",
    "tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load existing cached embeddings\n",
    "existing_pmids = []  # list of existing embedded pmids\n",
    "for f in tmp_dir.glob('*.safetensors'):\n",
    "  embeddings = load_file(f)\n",
    "  pmids = [int(k) for k in embeddings.keys()]\n",
    "  existing_pmids.extend(pmids)\n",
    "\n",
    "# filter out existing embeddings\n",
    "data = data.filter(~pl.col('pmid').is_in(existing_pmids))\n",
    "print(f'Loaded {len(existing_pmids)} existing embeddings.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18b7e8ba625c4d41aae211ac4dace65f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize the embedding pipeline\n",
    "model = SentenceTransformer(HF_MODEL_NAME, device='cuda').eval()\n",
    "model.max_seq_length = MAX_TOKENS\n",
    "\n",
    "# embed texts\n",
    "with torch.no_grad():\n",
    "  with tqdm(total=len(data)) as pbar:\n",
    "    n_embedded = 0\n",
    "    while n_embedded < len(data):\n",
    "      batch = data.slice(n_embedded, BATCH_SIZE)\n",
    "      texts = batch['abstract'].to_list()\n",
    "      pmids = batch.select(pl.col('pmid').cast(pl.Utf8))['pmid'].to_list()\n",
    "      e = model.encode(texts, convert_to_tensor=True)\n",
    "      e = dict(zip(pmids, e))\n",
    "      save_file(e, tmp_dir / f'from_{pmids[0]}.safetensors')  \n",
    "      n_embedded += BATCH_SIZE\n",
    "      pbar.update(BATCH_SIZE)\n",
    "\n",
    "# combine all embeddings into a single file and save\n",
    "if n_embedded > 0:\n",
    "  all_embeddings = {}\n",
    "  for f in tqdm(tmp_dir.glob('*.safetensors')):\n",
    "    embeddings = load_file(f)\n",
    "    all_embeddings.update(embeddings)\n",
    "  save_file(all_embeddings, OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9c6d8e22829943183dd37d6e0ea17534c0ca1a31e180ed9f59e3b2e78d2c52b1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
