{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit"
  },
  "interpreter": {
   "hash": "5ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# search and cache hits\n",
    "\n",
    "This notebook searches PubMed for all the defined queries in the ontology (i.e., properties of type `efo:pubmedQuery`),cleanups and aggregates the XML results, and stores all the search hits in a single CSV.\n",
    "\n",
    "**NOTE**: notebook should be executed from the project root folder"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## Output\n",
    "\n",
    "Aggregated abstracts are stored as a single compressed file: `data/pubmed_abstracts.csv.gz`. This output file contains the following features:\n",
    "\n",
    "- `category` (`str`): either `task` or `construct`\n",
    "- `subcategory` (`str`): task or construct name\n",
    "- `pmid` (`long`): PubMed Identifier\n",
    "- `doi` (`str`): DOI\n",
    "- `year` (`int`): publication year in `yyyy` format\n",
    "- `title` (`str`): publication title\n",
    "- `abstract` (`str`): publication abstract\n",
    "- `journal_title` (`str`): full journal title\n",
    "- `journal_iso_abbreviation` (`str`): Abbreviated journal title\n",
    "- `mesh` (`str`, deprecated): A list of Medical Subject Headings which indicates the field of research and other topics. We only keep major topics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "# install requirements\n",
    "!pip install owlready2 pandas dask[dataframe]\n",
    "\n",
    "# if you want to perform inference on the ontology (e.g., using hermit), then make sure to install java."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "zsh:1: no matches found: dask[dataframe]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from owlready2 import get_ontology\n",
    "import xmltodict\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "from python.cogtext import cleanup_abstract, search_and_store, extract_doi, parse_publication_year"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "DEBUG = True\n",
    "\n",
    "# collect data for the following categories\n",
    "CATEGORIES = ['CognitiveTask', 'CognitiveConstruct']\n",
    "\n",
    "OUTPUT_PATH = 'data/pubmed_abstracts.csv.gz'\n",
    "\n",
    "OWL_FILE = 'data/ontologies/efo.owl'\n",
    "ONTOLOGY = get_ontology(OWL_FILE).load()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "for category in CATEGORIES:\n",
    "  pubmed_queries = {e.name:e.pubmedQuery[0] for e in ONTOLOGY[category].descendants() if len(e.pubmedQuery) > 0}\n",
    "  print(f'EF ontology contains {len(pubmed_queries)} PubMed queries for {category}s.')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "EF ontology contains 126 PubMed queries for CognitiveTasks.\n",
      "EF ontology contains 72 PubMed queries for CognitiveConstructs.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "def search_and_cache_xml(pubmed_queries, overwrite_existing=False):\n",
    "    for subcategory, pubmed_query in pubmed_queries.items():\n",
    "        subcategory = subcategory.replace('/','') \n",
    "        fname = Path('data/pubmed/.cache') / (subcategory + '.xml')\n",
    "\n",
    "        if overwrite_existing or not fname.exists():\n",
    "            search_and_store(pubmed_query, fname)\n",
    "\n",
    "\n",
    "def convert_xml_to_csv(pubmed_queries, category, overwrite_existing=False):\n",
    "    \"\"\"cleanup and convert all abstracts into CSV files\"\"\"\n",
    "\n",
    "    for subcategory in sorted(pubmed_queries.keys()):\n",
    "\n",
    "        subcategory_fname = subcategory.replace('/','')\n",
    "\n",
    "        xml_file = Path('data/pubmed/.cache') / (subcategory_fname + '.xml')\n",
    "        csv_file = Path(f'data/pubmed/{category}') / (subcategory_fname + '.csv')\n",
    "\n",
    "        if xml_file.exists() and (overwrite_existing or not csv_file.exists()):\n",
    "            with open(xml_file, 'r') as f:\n",
    "\n",
    "                if DEBUG:\n",
    "                    print(f'[XML2CSV] converting \"{category}/{subcategory}\" dataset...')\n",
    "\n",
    "                xml_content = xmltodict.parse(f.read())\n",
    "\n",
    "                if 'PubmedArticleSet' in xml_content:\n",
    "\n",
    "                    df = pd.json_normalize(xml_content['PubmedArticleSet']['PubmedArticle'])\n",
    "\n",
    "                    # pmid, doi, title, and abstract\n",
    "                    df['pmid'] = df['MedlineCitation.PMID.#text']\n",
    "                    df['doi'] = df['PubmedData.ArticleIdList.ArticleId'].apply(extract_doi)\n",
    "                    df['title'] = df['MedlineCitation.Article.ArticleTitle']\n",
    "                    df['abstract'] = df['MedlineCitation.Article.Abstract.AbstractText'].apply(cleanup_abstract)\n",
    "                    \n",
    "                    # publication years\n",
    "                    df['year'] = df['MedlineCitation.Article.Journal.JournalIssue.PubDate.Year']\n",
    "                    df['journal_title'] = df['MedlineCitation.Article.Journal.Title']\n",
    "                    df['journal_iso_abbreviation'] = df['MedlineCitation.Article.Journal.ISOAbbreviation']\n",
    "\n",
    "                    # MeSh topics (some datasets do not contain MeshHeading, e.g., Spin The Pots)\n",
    "                    # if 'MedlineCitation.MeshHeadingList.MeshHeading' in df.columns:\n",
    "                    #     df['mesh'] = df['MedlineCitation.MeshHeadingList.MeshHeading'].apply(find_mesh)\n",
    "                    # else:\n",
    "                    #     df['mesh'] = None\n",
    "\n",
    "                    if 'MedlineCitation.Article.Journal.JournalIssue.PubDate.MedlineDate' in df.columns:\n",
    "                        medline_year = df['MedlineCitation.Article.Journal.JournalIssue.PubDate.MedlineDate'].apply(parse_publication_year)\n",
    "                        df['year'].fillna(medline_year, inplace=True)\n",
    "\n",
    "                    df['year'] = df['year'].apply(int)\n",
    "\n",
    "                    # fill missing abstracts with #text value\n",
    "                    if 'MedlineCitation.Article.Abstract.AbstractText.#text' in df.columns:\n",
    "                        df['abstract'].fillna(df['MedlineCitation.Article.Abstract.AbstractText.#text'], inplace=True)\n",
    "\n",
    "                    if 'MedlineCitation.Article.ArticleTitle.#text' in df.columns:\n",
    "                        df['title'].fillna(df['MedlineCitation.Article.ArticleTitle.#text'], inplace=True)\n",
    "\n",
    "                    # workaround to discard unusual terminators in the text\n",
    "                    df['abstract'] = df['abstract'].apply(lambda x: x.replace('\\u2029', ' ') if isinstance(x, str) else x)\n",
    "                    df['title'] = df['title'].apply(lambda x: x.replace('\\u2029', ' ') if isinstance(x, str) else x)\n",
    "\n",
    "                    df[['pmid', 'doi', 'year', 'journal_title', 'journal_iso_abbreviation', 'title','abstract']].to_csv(csv_file, index=False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "for category in CATEGORIES:\n",
    "\n",
    "    # init folder\n",
    "    Path(f'data/pubmed/{category}').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # fetch queries from the ontology\n",
    "    pubmed_queries = {e.name: e.pubmedQuery[0] for e in ONTOLOGY[category].descendants() if len(e.pubmedQuery) > 0}\n",
    "\n",
    "    search_and_cache_xml(pubmed_queries)\n",
    "    convert_xml_to_csv(pubmed_queries, category)\n",
    "\n",
    "print('Done!')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PubMed] query: (\"Oddity Switch\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Plus-Minus task\"[TIAB]) OR (\"Plus-Minus test\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Mister X task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Delay Alteration task\"[TIAB]) OR (\"Delay Alteration test\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Bear-Alligator task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Listening Recall\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Pick the Picture game\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Luria Hand Game\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Nebraska Barnyard\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"matrix span\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Digit-Shifting task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Expressive Attention task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Spatial Conflict Arrows\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"choice reaction time\" AND (test OR task) [TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Controlled Attention task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Serial Subtraction of Sevens\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"More-less/Odd-Even task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Animal Shifting\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Something's the Same game\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Pictorial Updating task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Matrix Monitoring task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Counting Recall\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Toy Sort\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Picture-Symbol task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Mental Counters task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "[PubMed] query: (\"Backwards Color Recall task\"[TIAB])\n",
      "[PubMed] no article found.\n",
      "Done!\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# aggregation\n",
    "import dask.dataframe as dd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "corpus_files = Path('data/pubmed/').glob('**/*.csv')\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for fname in tqdm(corpus_files):\n",
    "\n",
    "  # find categories from the file name\n",
    "  cats = re.findall('.*/pubmed/(.*)/(.*)\\\\.csv', str(fname))\n",
    "\n",
    "  # ignore other csv files\n",
    "  if len(cats) == 0:\n",
    "    continue\n",
    "\n",
    "  category = cats[0][0]\n",
    "  subcategory = cats[0][1]\n",
    "\n",
    "  df = dd.read_csv(fname)\n",
    "  df['category'] = category\n",
    "  df['subcategory'] = subcategory\n",
    "  dfs.append(df)\n",
    "\n",
    "# now aggregate all the corpora and store the compressed csv output (takez 85s on MacBook Pro 2018).\n",
    "dd.concat(dfs).to_csv(OUTPUT_PATH, single_file=True, index=False, compression='gzip')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "173it [00:02, 62.52it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['/Users/morteza/workspace/cogtext/data/pubmed_abstracts.csv.gz']"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  }
 ]
}