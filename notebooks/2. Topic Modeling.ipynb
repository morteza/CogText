{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ea20dcc4936bc3e336221d5e5ae7bcd09719da9b70640ea666426cf2d919dc3d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Custom preprocessing using NLTK\n",
    "\n",
    "This is just to verify the quality of gensim preprocessing pipeline. I will use gensim because of its reproducability and not this custom preprocessing."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/morteza/nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     /Users/morteza/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /Users/morteza/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[['execut', 'function', 'cool'],\n",
       " ['cognit', 'control', 'contain', 'sub', 'compon']]"
      ]
     },
     "metadata": {},
     "execution_count": 52
    }
   ],
   "source": [
    "import nltk\n",
    "import string   # for punctuation symbols.\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download('wordnet')#, '../data/cache/nltk')\n",
    "nltk.download('stopwords')#, '../data/cache/nltk')\n",
    "nltk.download('punkt')\n",
    "\n",
    "TEST_CORPUS = ['executive functions are cool!', 'cognitive control contains 9 sub-components!']\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess(doc):\n",
    "    # 1. lowercase\n",
    "    doc = doc.lower()\n",
    "    # 2. remove punctuations (TODO: and numbers)\n",
    "    doc = ''.join([c for c in doc if c not in string.punctuation])\n",
    "    # 3. tokenize\n",
    "    doc = nltk.word_tokenize(doc)\n",
    "    # 4. remove stopwords\n",
    "    doc = [w for w in doc if w not in stop_words]\n",
    "    # 5. stem\n",
    "    # doc = [stemmer.stem(w) for w in doc]\n",
    "    # 6. lemmatize\n",
    "    doc = [lemmatizer.lemmatize(w) for w in doc]\n",
    "    return doc\n",
    "\n",
    "# method 1\n",
    "[preprocess(doc) for doc in TEST_CORPUS]\n",
    "\n",
    "# method 2 (gensim)\n",
    "from gensim.parsing import preprocess_documents\n",
    "preprocess_documents(TEST_CORPUS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing import preprocess_documents, preprocess_string\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/pubmed/Anti Saccade.csv')\n",
    "df['abstract'].fillna(df['title'], inplace=True)\n",
    "\n",
    "df['preprocessed_abstract'] = df['abstract'].apply(lambda abstract: preprocess_string(abstract))\n",
    "# %timeit df['prep'] = preprocess_documents(df['abstract'].to_list())\n",
    "\n",
    "# Note: both methods preprocesses the corpus at the same speed (checked with %timeit)\n",
    "\n",
    "# DEBUG data check: df['preprocessed_abstract'], df.loc[0,'preprocessed_abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Letter Fluency 0\n",
      "Cognitive Control 87\n",
      "Auditory Attention 6\n",
      "Reverse Categorization 0\n",
      "Gift Wrap 0\n",
      "Grass Snow 0\n",
      "Verbal Fluency 16\n",
      "Random Letter Generation 0\n",
      "Digit Span 4\n",
      "Simon Says 1\n",
      "LNS 2\n",
      "Revised Attention Network Test 0\n",
      "Automated Operation Span 0\n",
      "Complex Span 1\n",
      "Incompatibility Test 0\n",
      "SR Compatibility 0\n",
      "Delay Discounting Task 1\n",
      "Number Letter 0\n",
      "Trail Making Task 0\n",
      "Semantic Fluency 3\n",
      "Flexible Item Selection 0\n",
      "Attention Network Test 0\n",
      "Head-Toes-Knees-Shoulders 0\n",
      "Spin the Pots 0\n",
      "Majority Function 0\n",
      "Balance Beam 2\n",
      "Simon 188\n",
      "MONITOR 62\n",
      "Delay Choice 0\n",
      "Counting Span 0\n",
      "Sentence Completion 1\n",
      "Letter Number Sequencing 0\n",
      "Iowa Gambling Task 4\n",
      "Block Span 0\n",
      "Modified Card Sorting Test 0\n",
      "Running Span 0\n",
      "Odd One Out 3\n",
      "Reversal Learning 9\n",
      "Self Control Schedule 0\n",
      "Category Switch 0\n",
      "BART 40\n",
      "Color Shape 0\n",
      "Random Number Generation 2\n",
      "Corsi Block Span 0\n",
      "Executive Function 79\n",
      "MOT 5\n",
      "Stroop 24\n",
      "Category Fluency 3\n",
      "OSpan 0\n",
      "N-back 2\n",
      "Multiple Object Tracking 1\n",
      "Wisconsin Card Sort 0\n",
      "Dragon 98\n",
      "Delayed Alternation 1\n",
      "Sorting 94\n",
      "Backward Span 1\n",
      "DCCS 0\n",
      "Letter Monitoring 1\n",
      "Spatial Span 0\n",
      "Anti Saccade 1\n",
      "OOO 2\n",
      "TMT 8\n",
      "Picture Symbol 0\n",
      "Visuospatial Span 0\n",
      "Cognitive Flexibility 13\n",
      "Attentional Network Test 0\n",
      "Verbal Span 0\n",
      "ANT 151\n",
      "Hayling Test 0\n",
      "Letter Memory 0\n",
      "IGT 32\n",
      "Sentence Repetition 1\n",
      "MFT 6\n",
      "Shape School 0\n",
      "STOP 496\n",
      "Simple Reaction Time 4\n",
      "Analogy Making 0\n",
      "Flexibility 47\n",
      "Gift Delay 0\n",
      "Pencil Tapping 0\n",
      "Dimension Switching 0\n",
      "Tower of London 3\n",
      "Stop Signal 6\n",
      "Attentional Blink 5\n",
      "Tower of Hanoi 0\n",
      "Snack Delay 0\n",
      "ANT-R 0\n",
      "WCST 2\n",
      "Flanker 1\n",
      "CPT 125\n",
      "Controlled Attention 0\n",
      "AX-CPT 0\n",
      "MCST 0\n",
      "Dimensional Change Card Sort 0\n",
      "Local Global 4\n",
      "Lexical Fluency 0\n",
      "GoNoGo 2\n",
      "Object Substitution 2\n",
      "Keep Track 19\n",
      "BCS 30\n",
      "Block Design Subtest 0\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "files = Path('../data/pubmed').glob('*.csv')\n",
    "\n",
    "for f in files:\n",
    "    df = pd.read_csv(f)\n",
    "    print(f.stem, df['abstract'].isna().sum())"
   ]
  }
 ]
}