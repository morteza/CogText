{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Topic Embedding\n",
    "\n",
    "This notebook implements an analysis that first produces a topic embedding for cognitive tasks and constructs. Topic embedding refers to the probabilities of assigning a topic to a given task/construct corpus. For example, task A could be assigned the following topic embedding: `[1., .5, .1]` which basically shows the probability of observing the three topics in the corpus A.\n",
    "\n",
    "## Data\n",
    "\n",
    "**Input**: `PUBMED` dataset contains the abstracts.\n",
    "\n",
    "**Output**: `topic_embeddings` is a table in which each row denotes an article and contains the following columns:\n",
    "\n",
    "- A list of topics\n",
    "- embeddings for each task/construct, i.e., probabilities of being assigned to topics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display\n",
    "\n",
    "from bertopic import BERTopic"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "source": [
    "dataset = 'pubmed5pct'\n",
    "version = '2021092511'\n",
    "model = BERTopic.load(f'outputs/models/{dataset}_bertopic_v{version}.model')\n",
    "\n",
    "probs_fpath = f'outputs/models/{dataset}_bertopic_v{version}.train_probs.npz'\n",
    "\n",
    "with np.load(probs_fpath) as probs_f:\n",
    "  probs = probs_f['arr_0']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "source": [
    "# with pd.option_context('display.max_rows', 10000):\n",
    "#   display(model.get_topic_info())\n",
    "\n",
    "# model.visualize_topics()\n",
    "\n",
    "# model.visualize_barchart()\n",
    "# model.visualize_distribution(probabilities=probs[0])\n",
    "\n",
    "model.get_params()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'calculate_probabilities': True,\n",
       " 'embedding_model': <bertopic.backend._sentencetransformers.SentenceTransformerBackend at 0x7f9d839d29d0>,\n",
       " 'hdbscan_model': HDBSCAN(min_cluster_size=10, prediction_data=True),\n",
       " 'language': 'english',\n",
       " 'low_memory': False,\n",
       " 'min_topic_size': 10,\n",
       " 'n_gram_range': (1, 1),\n",
       " 'nr_topics': None,\n",
       " 'seed_topic_list': None,\n",
       " 'top_n_words': 10,\n",
       " 'umap_model': UMAP(angular_rp_forest=True, dens_frac=0.0, dens_lambda=0.0, low_memory=False,\n",
       "      metric='cosine', min_dist=0.0, n_components=5),\n",
       " 'vectorizer_model': CountVectorizer(),\n",
       " 'verbose': True}"
      ]
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# RSA\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "sim_train = cosine_similarity(result.Z)\n",
    "sim_test = cosine_similarity(result.H_test)\n",
    "rho = spearmanr(sim_train, sim_test)\n",
    "print(f'[RSA] mean test/train correlation: {rho[0].mean():.2f}')\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('py3': conda)"
  },
  "interpreter": {
   "hash": "5ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}