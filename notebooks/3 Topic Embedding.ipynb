{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 3 Topic Embedding\n",
    "\n",
    "This notebook implements an analysis that first produces a topic embedding for cognitive tasks and constructs. Topic embedding refers to the probabilities of assigning a topic to a given task/construct corpus. For example, task A could be assigned the following topic embedding: `[1., .5, .1]` which basically shows the probability of observing the three topics in the corpus A.\n",
    "\n",
    "## Data\n",
    "\n",
    "**Input**: `PUBMED` dataset contains the abstracts.\n",
    "\n",
    "**Output**: `topic_embeddings` is a table in which each row denotes an article and contains the following columns:\n",
    "\n",
    "- A list of topics\n",
    "- embeddings for each task/construct, i.e., probabilities of being assigned to topics."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# load the data\n",
    "\n",
    "pubmed_path = Path('data/pubmed_abstracts_preprocessed.csv.gz')\n",
    "dataset = 'pubmed1pct'\n",
    "version = 'v202109291'\n",
    "\n",
    "indices = np.load(f'outputs/models/pubmed1pct_bertopic_{version}.idx.npz')['arr_0']\n",
    "model: BERTopic = BERTopic.load(f'outputs/models/pubmed1pct_bertopic_{version}.model')\n",
    "topics = np.load(f'outputs/models/pubmed1pct_bertopic_{version}.topics.npz')['arr_0']\n",
    "probs = np.load(f'outputs/models/pubmed1pct_bertopic_{version}.probs.npz')['arr_0']\n",
    "\n",
    "data = pd.read_csv(pubmed_path)\n",
    "data = data[data.index.isin(indices)]\n",
    "\n",
    "# DEBUG\n",
    "# with pd.option_context('display.max_rows', 10000):\n",
    "#   display(model.get_topic_info())\n",
    "\n",
    "# model.visualize_topics()\n",
    "\n",
    "# model.visualize_barchart()\n",
    "# model.visualize_distribution(probabilities=probs[0])\n",
    "\n",
    "# model.get_params()\n",
    "# model.get_topics()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_topics = 300\n",
    "batch_size = 200\n",
    "n_samples = 10000\n",
    "n_labels = 2\n",
    "\n",
    "X, y = make_classification(n_samples=n_samples, n_features=n_topics, n_classes=n_labels, random_state=0)\n",
    "\n",
    "X = torch.tensor(X).type(torch.float)\n",
    "y = torch.tensor(y).type(torch.long)\n",
    "\n",
    "class TopicEmbeddingNet(nn.Module):\n",
    "  def __init__(self, n_topics: int, n_labels: int):\n",
    "    super(TopicEmbeddingNet, self).__init__()\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Linear(n_topics, n_topics), nn.BatchNorm1d(196), nn.LeakyReLU(0.1),\n",
    "        nn.Linear(196, 128), nn.BatchNorm1d(128), nn.LeakyReLU(0.1),\n",
    "        nn.Linear(128, n_topics)\n",
    "    )\n",
    "    self.hidden2mu = nn.Linear(n_topics, n_topics)\n",
    "    self.hidden2log_var = nn.Linear(n_topics, n_topics)\n",
    "    self.decoder = nn.Sequential(\n",
    "      nn.Linear(n_topics, 128), nn.BatchNorm1d(128), nn.LeakyReLU(0.1),\n",
    "      nn.Linear(128, 196), nn.BatchNorm1d(196), nn.LeakyReLU(0.1),\n",
    "      nn.Linear(196, n_topics),\n",
    "    )\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "      nn.Linear(n_topics, n_topics), nn.BatchNorm1d(196), nn.LeakyReLU(0.1)\n",
    "    )\n",
    "    self.embedding = nn.Embedding(n_labels, n_topics)\n",
    "\n",
    "  def encode(self, x, label):\n",
    "    # h = self.encoder(x)\n",
    "    # mu = self.hidden2mu(h)\n",
    "    # log_var = self.hidden2log_var(h)\n",
    "    # sigma = torch.exp(0.5*log_var)\n",
    "    # z = torch.randn_like(sigma)\n",
    "    # h = mu + sigma * z\n",
    "    # return mu, log_var, h\n",
    "\n",
    "    x = self.encoder(x)\n",
    "    h = self.embedding(label)\n",
    "\n",
    "  def decode(self, x):\n",
    "    x = self.decoder(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    mu, log_var, h = self.encode(x)\n",
    "    y = self.decoder(h)\n",
    "    return mu, log_var, y\n",
    "\n",
    "model = TopicEmbeddingNet(n_topics, n_labels)\n",
    "\n",
    "print('before train:', (model(X)[2].argmax(dim=1) == y).sum().item())\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in tqdm(range(3)):\n",
    "  model.train()\n",
    "  model.zero_grad()\n",
    "  _, _, x_pred = model(X)\n",
    "  loss = criterion(x_pred, X)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # TODO eval\n",
    "\n",
    "print('after train:', (model(X)[2].argmax(dim=1) == y).sum().item())"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "before train: 47\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.90it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "after train: 34\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "print(dict(model.named_parameters())['hidden2mu.weight'].shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "torch.Size([300, 300])\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "import skorch\n",
    "from skorch.callbacks import TensorBoard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "cols = ['category', 'subcategory']\n",
    "X = np.vstack([data[col].astype('category').cat.codes for col in cols]).T\n",
    "y = probs\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.int)\n",
    "y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "\n",
    "net = skorch.NeuralNetRegressor(\n",
    "  TopicEmbeddingNet(1,1,1),\n",
    "  max_epochs=100,\n",
    "  lr=0.1,\n",
    "  iterator_train__shuffle=True,\n",
    "  # DEBUG: callbacks=[TensorBoard(writer=SummaryWriter())]\n",
    ")\n",
    "\n",
    "# DEBUG: net.fit(X, y)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "  'lr': [0.01],\n",
    "  'max_epochs': [100],\n",
    "  'module__n_cats': [data['category'].nunique()],\n",
    "  'module__n_subcats': [data['subcategory'].nunique()],\n",
    "  'module__embeddings_dim': range(1, y.shape[1]),\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(net, params, scoring='accuracy')\n",
    "\n",
    "gs.fit(X, y)\n",
    "print(gs.best_score_, gs.best_params_, gs.best_estimator_)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "model = gs.best_params_\n",
    "\n",
    "gs.estimator\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   params = list(model.get_params()['module'].parameters())\n",
    "#   cat_embeddings = params[0]\n",
    "#   subcat_embeddings = params[1]\n",
    "\n",
    "# model.save_params(f_params='model.pkl')"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
       "  module=TopicEmbeddingNet(\n",
       "    (cat_embedding): Embedding(1, 1)\n",
       "    (subcat_embedding): Embedding(1, 1)\n",
       "    (fc): Linear(in_features=1, out_features=65, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# test/train RSA\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "sim_train = cosine_similarity(model.topic_embeddings)\n",
    "sim_test = cosine_similarity(result.H_test)\n",
    "rho = spearmanr(sim_train, sim_test)\n",
    "print(f'[RSA] mean test/train correlation: {rho[0].mean():.2f}')\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.4",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('py3': conda)"
  },
  "interpreter": {
   "hash": "5ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}