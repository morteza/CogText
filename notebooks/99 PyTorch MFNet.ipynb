{"cells":[{"cell_type":"code","execution_count":1,"source":["%reload_ext autoreload\n","%autoreload 2\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, explained_variance_score, r2_score\n","\n","from tqdm import tqdm\n","\n","import torch\n","from torch import nn\n","from torch.utils.data import TensorDataset, DataLoader, random_split\n","from torch.utils.tensorboard import SummaryWriter\n","\n","from python.cogtext.utils import select_relevant_pubmed_articles\n","from python.cogtext.coappearance_matrix import generate_coappearance_matrix_fast\n","\n","sns.set()"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":2,"source":["# parameters\n","DEV_MODE = True\n","INPUT_FILE = 'data/pubmed_abstracts.csv.gz'"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":3,"source":["# prepare data\n","\n","PUBMED = (pd.read_csv(INPUT_FILE)\n","            .pipe(select_relevant_pubmed_articles)\n","            .dropna(subset=['abstract']))\n","\n","# only corpora with # of articles < DEV_MAX_CORPUS_SIZE\n","# subcats_cnt = PUBMED['subcategory'].value_counts()\n","# small_subcats = subcats_cnt[subcats_cnt < DEV_MAX_CORPUS_SIZE].index.to_list()\n","# PUBMED = PUBMED.query('subcategory in @small_subcats',).copy()\n","\n","# DROP tasks/constructs with less than 5 articles (1/test + 1/valid + 4/train = 6)\n","valid_subcats = PUBMED['subcategory'].value_counts()[lambda cnt: cnt > 5].index.to_list()\n","PUBMED = PUBMED.query('subcategory in @valid_subcats')\n","\n","# train/test split (80% train 20% test)\n","PUBMED_train, PUBMED_test = train_test_split(\n","    PUBMED,\n","    test_size=0.2,\n","    stratify=PUBMED['subcategory'],\n","    random_state=0)\n","\n","n_constructs = PUBMED.groupby('category')['subcategory'].nunique()['CognitiveConstruct']\n","n_tasks = PUBMED.groupby('category')['subcategory'].nunique()['CognitiveTask']"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":21,"source":["class MFNet(nn.Module):\n","  def __init__(self, n_tasks, n_constructs, n_embeddings):\n","    super(MFNet, self).__init__()\n","    self.task_embeddings = nn.Embedding(n_tasks, n_embeddings)\n","    self.construct_embeddings = nn.Embedding(n_constructs, n_embeddings)\n","    self.task_biases = torch.nn.Embedding(n_tasks, 1)\n","    self.construct_biases = torch.nn.Embedding(n_constructs, 1)\n","    self.decoder = nn.Linear(n_embeddings, 1)\n","\n","  def forward(self, x):\n","    construct, task = x[:, 0].type(torch.int), x[:, 1].type(torch.int)\n","    M = self.task_embeddings(task)\n","    C = self.construct_embeddings(construct)\n","    bias = self.task_biases(task) + self.construct_biases(construct)\n","    y = torch.diagonal(M @ C.T).unsqueeze(1) + bias\n","    # y = self.decoder(H)\n","    return y\n","\n","  def fit(self,\n","      X, y,\n","      train_split_size=.8,\n","      n_epochs=1000,\n","      batch_size=100,\n","      logger: SummaryWriter=SummaryWriter()):\n","\n","    assert 0. < train_split_size < 1.0\n","\n","    n_samples = X.shape[0]\n","\n","    train_size = int(n_samples * train_split_size)\n","    test_size = n_samples - train_size\n","\n","\n","    dataset = TensorDataset(X, y)\n","    train_subset, test_subset = random_split(dataset, lengths=(train_size,test_size))\n","\n","    X_test, y_test = dataset[test_subset.indices]\n","\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam(self.parameters(), lr=.001)\n","\n","    logger.add_graph(self, X)\n","\n","    for epoch in tqdm(range(n_epochs)):\n","\n","      # train model\n","      self.train()\n","      for X_batch, y_batch in DataLoader(train_subset, batch_size=batch_size):\n","        self.zero_grad()\n","\n","        # X_train = torch.tensor(X).type(torch.int)\n","        # y_train = torch.tensor(y).type(torch.float)\n","        # y_pred = self(X_train[:, 0], X_train[:, 1])\n","\n","        y_pred = self(X_batch)\n","        loss = criterion(y_batch, y_pred)\n","        logger.add_scalar('loss/train', loss.detach(), epoch)\n","        loss.backward()\n","        optimizer.step()\n","\n","      # eval mode\n","      self.eval()\n","      with torch.no_grad():\n","        y_pred = self(X_test)\n","        loss = criterion(y_test, y_pred)\n","        logger.add_scalar('loss/test', loss.detach(), epoch)\n","        \n","        # ev = explained_variance_score(y_test, y_pred)\n","        # logger.add_scalar('explained_variance/test', ev, epoch)\n","\n","    return self"],"outputs":[],"metadata":{}},{"cell_type":"code","execution_count":22,"source":["# TODO\n","cols = ['construct','task']\n","\n","# create CO_APPEARANCE\n","CO_APPEARANCE = PUBMED.pipe(generate_coappearance_matrix_fast, probability=True, group_categories=True)\n","X = np.vstack([CO_APPEARANCE[c].astype('category').cat.codes for c in cols]).T\n","y = CO_APPEARANCE[['probability']].values\n","\n","# TODO drop 0 probabilities from X\n","# TODO n_embeddings should be a hyper parameter (use Ax to optimize)\n","\n","model = MFNet(n_tasks, n_constructs, 7)\n","model.fit(torch.tensor(X, dtype=torch.float), torch.tensor(y, dtype=torch.float))\n","\n","# %reload_ext tensorboard\n","# %tensorboard --logdir=runs/"],"outputs":[{"output_type":"stream","name":"stderr","text":[" 50%|█████     | 505/1000 [00:51<00:58,  8.50it/s]"]}],"metadata":{}}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.4"},"orig_nbformat":4,"kernelspec":{"name":"python3","display_name":"Python 3.9.4 64-bit ('py3': conda)"},"interpreter":{"hash":"5ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84"}}}