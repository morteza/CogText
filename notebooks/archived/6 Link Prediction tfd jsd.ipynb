{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jenssen Shannon Distance using Tensorflow Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping...\n",
    "\n",
    "# KL model\n",
    "def nll(X, dist):\n",
    "  return - tf.reduce_mean(dist.log_prob(X))\n",
    "\n",
    "@tf.function\n",
    "def get_loss_and_grads(X_train, dist):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(dist.trainable_variables)\n",
    "    loss = nll(X_train, dist)\n",
    "  grads = tape.gradient(loss, dist.trainable_variables)\n",
    "  return loss, grads\n",
    "\n",
    "def fit_multivariate_normal(data, n_epochs=10, batch_size=100):\n",
    "  dist = tfd.MultivariateNormalDiag(\n",
    "    loc=tf.Variable(data.mean(axis=0), name='loc'),\n",
    "    scale_diag=tf.Variable(np.ones(data.shape[1]), name='scale_diag'))\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "  for _ in range(n_epochs):\n",
    "    # for batch in np.array_split(data, 1 + (data.shape[0] // batch_size)):\n",
    "    batch = data  # use all the data in each epoch\n",
    "    loss, grads = get_loss_and_grads(batch, dist)\n",
    "    optimizer.apply_gradients(zip(grads, dist.trainable_variables))\n",
    "    # loc_value = dist.loc.value()\n",
    "  return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%script echo skipping...\n",
    "\n",
    "# node2node js distance matrix\n",
    "n2n_jsd = np.zeros((len(node_dists), len(node_dists)))\n",
    "node_means = {}\n",
    "node_stds = {}\n",
    "\n",
    "for i,lbl_i in tqdm(enumerate(node_dists.index)):\n",
    "  for j, lbl_j in enumerate(node_dists.index):\n",
    "    P = node_dists.loc[lbl_i]\n",
    "    Q = node_dists.loc[lbl_j]\n",
    "    M = tfd.MultivariateNormalDiag(\n",
    "      loc=(P.mean() + Q.mean())/2.,\n",
    "      scale_diag=(P.stddev() + Q.stddev())/2.\n",
    "    )\n",
    "    node_means[lbl_i] = P.mean().numpy()\n",
    "    node_stds[lbl_i] = P.stddev().numpy()\n",
    "    jsd = .5 * (P.kl_divergence(M) + Q.kl_divergence(M)).numpy()\n",
    "    # print(X_dists.loc[lbl_i].trainable_variables[1].shape)\n",
    "    n2n_jsd[i,j] = jsd\n",
    "\n",
    "n2n_jsd = pd.DataFrame(n2n_jsd, index=node_dists.index.to_list(), columns=node_dists.index.to_list())\n",
    "# node_dists = node_features.progress_apply(fit_multivariate_normal)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
