{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jenssen Shannon Distance using Tensorflow Probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script echo skipping...\n",
    "\n",
    "# KL model\n",
    "def nll(X, dist):\n",
    "  return - tf.reduce_mean(dist.log_prob(X))\n",
    "\n",
    "@tf.function\n",
    "def get_loss_and_grads(X_train, dist):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(dist.trainable_variables)\n",
    "    loss = nll(X_train, dist)\n",
    "  grads = tape.gradient(loss, dist.trainable_variables)\n",
    "  return loss, grads\n",
    "\n",
    "def fit_multivariate_normal(data, n_epochs=10, batch_size=100):\n",
    "  dist = tfd.MultivariateNormalDiag(\n",
    "    loc=tf.Variable(data.mean(axis=0), name='loc'),\n",
    "    scale_diag=tf.Variable(np.ones(data.shape[1]), name='scale_diag'))\n",
    "\n",
    "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.05)\n",
    "\n",
    "  for _ in range(n_epochs):\n",
    "    # for batch in np.array_split(data, 1 + (data.shape[0] // batch_size)):\n",
    "    batch = data  # use all the data in each epoch\n",
    "    loss, grads = get_loss_and_grads(batch, dist)\n",
    "    optimizer.apply_gradients(zip(grads, dist.trainable_variables))\n",
    "    # loc_value = dist.loc.value()\n",
    "  return dist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%script echo skipping...\n",
    "\n",
    "# node2node js distance matrix\n",
    "n2n_jsd = np.zeros((len(node_dists), len(node_dists)))\n",
    "node_means = {}\n",
    "node_stds = {}\n",
    "\n",
    "for i,lbl_i in tqdm(enumerate(node_dists.index)):\n",
    "  for j, lbl_j in enumerate(node_dists.index):\n",
    "    P = node_dists.loc[lbl_i]\n",
    "    Q = node_dists.loc[lbl_j]\n",
    "    M = tfd.MultivariateNormalDiag(\n",
    "      loc=(P.mean() + Q.mean())/2.,\n",
    "      scale_diag=(P.stddev() + Q.stddev())/2.\n",
    "    )\n",
    "    node_means[lbl_i] = P.mean().numpy()\n",
    "    node_stds[lbl_i] = P.stddev().numpy()\n",
    "    jsd = .5 * (P.kl_divergence(M) + Q.kl_divergence(M)).numpy()\n",
    "    # print(X_dists.loc[lbl_i].trainable_variables[1].shape)\n",
    "    n2n_jsd[i,j] = jsd\n",
    "\n",
    "n2n_jsd = pd.DataFrame(n2n_jsd, index=node_dists.index.to_list(), columns=node_dists.index.to_list())\n",
    "# node_dists = node_features.progress_apply(fit_multivariate_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import  scipy.stats\n",
    "import numpy as np\n",
    "# scipy.stats.gaussian_kde([[1,2,3],[2,4,4]] + np.finfo(float).eps).pdf(0.5)\n",
    "n_topics = 7\n",
    "n_data = 100\n",
    "X = np.random.rand(n_data, n_topics)\n",
    "# scipy.stats.gaussian_kde(X.T).pdf([0] * n_topics)\n",
    "# scipy.stats.norm(X.mean(axis=0), X.std(axis=0)).pdf([0] * n_topics)\n",
    "\n",
    "# dist = scipy.stats.multivariate_normal(X.mean(axis=0), X.std(axis=0))\n",
    "# dist.pdf([[0] * n_topics, [.5] * n_topics])\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "tfd = tfp.distributions\n",
    "\n",
    "P = tfd.Empirical(X[:10], event_ndims=1)\n",
    "# P = tfd.MultivariateNormalDiag(X[:10].mean(axis=0), X[:10].std(axis=0))\n",
    "# Q = tfd.MultivariateNormalDiag(X[10:].mean(axis=0), X[10:].std(axis=0))\n",
    "P.prob([[0] * n_topics])\n",
    "\n",
    "\n",
    "# empirical KLd\n",
    "def empirical_kld(p_data, q_data):\n",
    "    from scipy.spatial import KDTree\n",
    "\n",
    "    n_p = p_data.shape[0]\n",
    "    n_q = p_data.shape[1]\n",
    "\n",
    "    # dim = n_topics\n",
    "\n",
    "    r = KDTree(p_data).query(p_data, k=2, eps=.01, p=2)[0][:,1]\n",
    "    s = KDTree(q_data).query(p_data, k=1, eps=.01, p=2)[0]\n",
    "\n",
    "    kld = -np.log(r/s).sum() * n_topics / n_p + np.log(n_q / (n_p - 1.))\n",
    "    return kld\n",
    "\n",
    "empirical_kld(X[:10], X[10:]), empirical_kld(X[10:], X[:10])\n",
    "\n",
    "# np.mgrid.__getitem__([slice(0,1,.1)] * n_topics).reshape(n_topics,-1).T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {'bandwidth': np.logspace(-1, 1, 20)}\n",
    "grid = GridSearchCV(KernelDensity(), params)\n",
    "grid.fit(X)\n",
    "kde = grid.best_estimator_\n",
    "kde.score_samples(X)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
