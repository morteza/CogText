{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. PubMed Abstracts Annotator\n",
    "\n",
    "This notebooks implements a graphical interface to annotate PubMed abstracts by marking articles as relevant or irrelevant to a given category.\n",
    "\n",
    "First, run all the cells up to the last one. Last cell only saves the annotated outputs into the `data/pubmed_abstracts_annotations.csv.gz` file. You don't need to annotate all the articles at once. Whenever you run the notebook, it only shows those articles that are not already annotated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pigeon import annotate\n",
    "import pigeonXT as pixt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus\n",
    "\n",
    "The following loads all the CSV files from the `data/pubmed/tests/` directory and prepares them to be annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading CSV files: 91it [00:05, 15.57it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not Path('data/pubmed/annotated_cognitive_tests_corpus.csv').exists():\n",
    "    csv_files = Path('data/pubmed').glob('tests/*.csv')\n",
    "\n",
    "    corpora = []\n",
    "    for csv_file in tqdm(csv_files, desc='Reading CSV files'):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['corpus_name'] = csv_file.stem\n",
    "        corpora.append(df)\n",
    "\n",
    "    df = pd.concat(corpora, axis=0)\n",
    "    df['abstract'].fillna(df['title'], inplace=True)\n",
    "    df['is_annotated'] = False\n",
    "    df['annotation'] = None\n",
    "    df.to_csv('data/pubmed/annotated_cognitive_tests_corpus.csv', index=False, na_rep='n/a')\n",
    "else:\n",
    "    # load the aggregated corpus if the aggregated file already exists\n",
    "    df = pd.read_csv('data/pubmed/annotated_cognitive_tests_corpus.csv')\n",
    "\n",
    "# workaround to discard unusual terminators in the text\n",
    "df['abstract'] = df['abstract'].apply(lambda x: x.replace('\\u2029', ' ') if isinstance(x, str) else x)\n",
    "df['title'] = df['title'].apply(lambda x: x.replace('\\u2029', ' ') if isinstance(x, str) else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Annonate PubMed corpus\n",
    "\n",
    "For each article, you will be asked to annotate them as `relevant` or `irrelevant`. You need to make sure of the following constraints for each question:\n",
    "\n",
    "- `journal_title` refers to a cogntive science journal.\n",
    "- `title` and `abstract` are in theory and practice related to the context that is provided in the `corpus_name`.\n",
    "- and all the other features make sense.\n",
    "\n",
    "You will have three choices: relevant, irrelevant, and skip.\n",
    "\n",
    "When you are done with annotating, just run the next cell to store your work. Don't worry; next time you run this annotating process, you will only see those articles that are not previously annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49c66c19690f4095bb4a7839d7b9057d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HTML(value='0 of 91229 Examples annotated, Current Position: 0 ')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "877b957b09c94087a9e8e9dd27e33cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(description='relevant', style=ButtonStyle()), Button(description='irrelevant', style=Butâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4b2875a54484d00a2edd1745b261f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def store_annotations(annotations, path = 'data/pubmed/annotated_cognitive_tests_corpus.csv'):\n",
    "  df.loc[list(annotations['row_index']), 'is_annotated'] = annotations['changed']\n",
    "  df.loc[list(annotations['row_index']), 'annotation'] = annotations['annotation']\n",
    "  df.to_csv(path, index=False, na_rep='n/a')\n",
    "\n",
    "\n",
    "annotations = pixt.annotate(\n",
    "  df.query('is_annotated.isna() or (is_annotated == False)').index.to_list(),\n",
    "  options=['relevant','irrelevant'],\n",
    "  # task_type='multilabel-classification',\n",
    "  # shuffle=True,\n",
    "  buttons_in_a_row=5,\n",
    "  example_column='row_index',\n",
    "  value_column='annotation',\n",
    "  display_fn=lambda x: display(x,df[x:x+1].T),\n",
    "  final_process_fn=lambda annotations: store_annotations(annotations)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the newly annonated articles\n",
    "\n",
    "**do NOT run this cell except when you're done annotating and want to save the results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_annotations(annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bertopic import BERTopic\n",
    "\n",
    "version = 'v202109291'\n",
    "\n",
    "indices = np.load(f'models/pubmed1pct_bertopic_{version}.idx.npz')['arr_0']\n",
    "model: BERTopic = BERTopic.load(f'models/pubmed1pct_bertopic_{version}.model')\n",
    "topics = np.load(f'models/pubmed1pct_bertopic_{version}.topics.npz')['arr_0']\n",
    "probs = np.load(f'models/pubmed1pct_bertopic_{version}.probs.npz')['arr_0']\n",
    "\n",
    "PUBMED = pd.read_csv('data/pubmed_abstracts_preprocessed.csv.gz')\n",
    "PUBMED = PUBMED[PUBMED.index.isin(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3146, 65)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.reduce_topics(docs = PUBMED['abstract'].to_list(), topics=topics, probabilities=probs)\n",
    "# model.get_representative_docs(1)\n",
    "# model.get_topic(1)\n",
    "probs.shape"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('py3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
