{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    ":warning: Due to the changes in PubMed API, this notebook does not work as expected anymore :warning:\n",
    "# Search PubMed and store abstracts\n",
    "\n",
    "This notebook searches PubMed for all the defined queries in the EFO ontology (i.e., properties of type `efo:pubmedQuery`), then cleanups and aggregates the XML results, and stores all the search hits in a single CSV.\n",
    "\n",
    "**NOTE**: Notebooks should be executed from the project root folder. All paths are relative to the project root.\n",
    "\n",
    "\n",
    "## Input\n",
    "\n",
    "- `data/ontologies/efo.owl`: EFO ontology.\n",
    "\n",
    "## Output\n",
    "\n",
    "- `data/pubmed/abstracts_2023.csv.gz`: Aggregated abstracts are stored as a single compressed file, containing the following columns:\n",
    "    - `category` (`str`): either `CognitiveTask` or `CognitiveConstruct`\n",
    "    - `subcategory` (`str`): task or construct name\n",
    "    - `pmid` (`long`): PubMed identifier\n",
    "    - `doi` (`str`): DOI\n",
    "    - `year` (`int`): publication year in `yyyy` format\n",
    "    - `title` (`str`): publication title\n",
    "    - `abstract` (`str`): publication abstract\n",
    "    - `journal_title` (`str`): full journal title\n",
    "    - `journal_iso_abbreviation` (`str`): Abbreviated journal title\n",
    "    - `mesh` (`str`, deprecated): A list of Medical Subject Headings which indicates the field of research and other topics. We only keep major topics.\n",
    "\n",
    "## Requirements\n",
    "\n",
    "First set the `NCBI_API_KEY` environment variable ([How do I obtain an API Key through an NCBI account\n",
    "](https://support.nlm.nih.gov/knowledgebase/article/KA-05317/en-us)). Then activate the `cogtext` conda environment:\n",
    "\n",
    "```bash\n",
    "mamba activate cogtext\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 3\n",
    "\n",
    "from owlready2 import get_ontology\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv; load_dotenv()  # to load NCBI_API_KEY env variable\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from src.cogtext.datasets.pubmed import convert_xml_to_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True\n",
    "\n",
    "# collect data for the following categories\n",
    "CATEGORIES = ['CognitiveTask', 'CognitiveConstruct']\n",
    "\n",
    "OUTPUT_PATH = 'data/pubmed/abstracts_2023.csv.gz'\n",
    "\n",
    "OWL_FILE = 'data/ontologies/efo.owl'\n",
    "ONTOLOGY = get_ontology(OWL_FILE).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EF ontology contains 126 PubMed queries for CognitiveTasks.\n",
      "EF ontology contains 72 PubMed queries for CognitiveConstructs.\n"
     ]
    }
   ],
   "source": [
    "for category in CATEGORIES:\n",
    "  pubmed_queries = {e.name:e.pubmedQuery[0] for e in ONTOLOGY[category].descendants() if len(e.pubmedQuery) > 0}\n",
    "  print(f'EF ontology contains {len(pubmed_queries)} PubMed queries for {category}s.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EDirect] Finished (\\\"Backwards Color Recall task\\\"[TIAB])\n",
      "[EDirect] Finished (\\\"Toy Sort\\\"[TIAB])\n",
      "[EDirect] Finished (Degraded Vigilance task* \\\"PVT\\\"[TIAB])\n",
      "[EDirect] Finished (\\\"Balance Beam task\\\"[TIAB]) OR (\\\"Balance Beam test\\\"[TIAB])\n",
      "[EDirect] Finished (\\\"Lexical Fluency\\\"[TIAB])\n",
      "[EDirect] Finished (\\\"Controlled Attention task\\\"[TIAB])\n",
      "[EDirect] Finished (\\\"snack delay\\\"[TIAB]) AND (task* OR test* OR paradigm*[TIAB])\n",
      "[EDirect] Finished \\\"Stimulus Selective Stop Signal Task\\\"[TIAB]\n",
      "[EDirect] Finished (\\\"Counting Span\\\"[TIAB])\n",
      "[EDirect] Finished (\\\"Self Control Schedule\\\"[TIAB])\n",
      "[EDirect] Finished (\\\"Pencil Tapping\\\"[TIAB]) OR (\\\"PEG task\\\"[TIAB]) OR (\\\"PEG test\\\"[TIAB])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/morteza/edirect/efetch: line 1116: 695991 Done                    echo \"$res\"\n",
      "     695992                       | join-into-groups-of \"$chunk\"\n",
      "     695994 Killed                  | while read uids; do\n",
      "    RunWithFetchArgs nquire -url \"$base\" efetch.fcgi -db \"$dbase\" -id \"$uids\" -rettype \"$format\" -retmode \"$mode\";\n",
      "done\n",
      "/home/morteza/edirect/efetch: line 1116: 695130 Exit 137                if [ \"$needHistory\" = false ]; then\n",
      "    res=$(LimitUidList \"$dbase\"); echo \"$res\" | join-into-groups-of \"$chunk\" | while read uids; do\n",
      "        RunWithFetchArgs nquire -url \"$base\" efetch.fcgi -db \"$dbase\" -id \"$uids\" -rettype \"$format\" -retmode \"$mode\";\n",
      "    done;\n",
      "else\n",
      "    GenerateHistoryChunks \"$chunk\" \"$min\" \"$max\" | while read fr chnk; do\n",
      "        RunWithFetchArgs nquire -url \"$base\" efetch.fcgi -query_key \"$qry_key\" -WebEnv \"$web_env\" -retstart \"$fr\" -retmax \"$chnk\" -db \"$dbase\" -rettype \"$format\" -retmode \"$mode\";\n",
      "    done;\n",
      "fi\n",
      "     695131 Killed                  | if [ \"$format\" = \"json\" ] || [ \"$mode\" = \"json\" ] || [ \"$raw\" = true ]; then\n",
      "    grep '.';\n",
      "else\n",
      "    if [ \"$json\" = true ]; then\n",
      "        transmute -x2j;\n",
      "    else\n",
      "        if [ \"$isFasta\" = true ]; then\n",
      "            grep '.';\n",
      "        else\n",
      "            if [ \"$format\" = \"full\" ] && [ \"$mode\" = \"xml\" ]; then\n",
      "                if [ \"$dbase\" = \"pubmed\" ] && [ \"$isPreview\" = true ]; then\n",
      "                    if [ \"$pubmedAPA\" = true ]; then\n",
      "                        transmute -normalize \"$dbase\" | pma2apa -ascii;\n",
      "                    else\n",
      "                        if [ \"$pubmedASN\" = true ]; then\n",
      "                            transmute -normalize \"$dbase\" | pma2pme -std;\n",
      "                        else\n",
      "                            transmute -normalize \"$dbase\" | transmute -normalize \"$dbase\" | transmute -format indent -combine -doctype \"\";\n",
      "                        fi;\n",
      "                    fi;\n",
      "                else\n",
      "                    if [ \"$dbase\" = \"pubmed\" ]; then\n",
      "                        transmute -normalize \"$dbase\" | transmute -format indent -combine -doctype \"\";\n",
      "                    else\n",
      "                        if [ \"$dbase\" = \"sra\" ]; then\n",
      "                            transmute -normalize \"$dbase\" | transmute -format indent -combine -doctype \"\" -self;\n",
      "                        else\n",
      "                            if [ \"$dbase\" = \"pmc\" ]; then\n",
      "                                grep '.';\n",
      "                            else\n",
      "                                if [ \"$dbase\" = \"pccompound\" ] || [ \"$dbase\" = \"pcsubstance\" ]; then\n",
      "                                    transmute -mixed -normalize \"$dbase\";\n",
      "                                else\n",
      "                                    transmute -format indent -combine -doctype \"\";\n",
      "                                fi;\n",
      "                            fi;\n",
      "                        fi;\n",
      "                    fi;\n",
      "                fi;\n",
      "            else\n",
      "                if [ \"$dbase\" = \"sra\" ] && [ \"$format\" = \"runinfo\" ]; then\n",
      "                    sed '2,${/Run,ReleaseDate,LoadDate/d;}';\n",
      "                else\n",
      "                    grep '';\n",
      "                fi;\n",
      "            fi;\n",
      "        fi;\n",
      "    fi;\n",
      "fi\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/morteza/workspace/cogtext/notebooks/1 Data Collection.ipynb Cell 6\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     csv_dir\u001b[39m.\u001b[39mmkdir(parents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     queries \u001b[39m=\u001b[39m {subc\u001b[39m.\u001b[39mname: subc\u001b[39m.\u001b[39mpubmedQuery[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m                \u001b[39mfor\u001b[39;00m subc \u001b[39min\u001b[39;00m ONTOLOGY[category]\u001b[39m.\u001b[39mdescendants()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m                \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(subc\u001b[39m.\u001b[39mpubmedQuery) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m}\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     Parallel(n_jobs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m)(delayed(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m         pubmed_pipeline\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m     )(subcategory, query, csv_dir\u001b[39m=\u001b[39;49mcsv_dir, overwrite\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m                        \u001b[39mfor\u001b[39;49;00m subcategory, query \u001b[39min\u001b[39;49;00m queries\u001b[39m.\u001b[39;49mitems())\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m \u001b[39m# clear_output()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/morteza/workspace/cogtext/notebooks/1%20Data%20Collection.ipynb#W4sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mDone!\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/micromamba/envs/cogtext/lib/python3.11/site-packages/joblib/parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[1;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[0;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39;49m(output)\n",
      "File \u001b[0;32m~/micromamba/envs/cogtext/lib/python3.11/site-packages/joblib/parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[1;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/micromamba/envs/cogtext/lib/python3.11/site-packages/joblib/parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[1;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[1;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[1;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[1;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "import shlex\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "\n",
    "def pubmed_pipeline(subcategory, query, csv_dir, overwrite=False,\n",
    "                    edirect_dir='/home/morteza/edirect'):\n",
    "    subcategory = subcategory.replace('/', '')\n",
    "    fname = Path('data/pubmed/.cache') / (subcategory + '.asn')\n",
    "\n",
    "    if overwrite or not fname.exists():\n",
    "        edirect_env = os.environ.copy()\n",
    "        edirect_env['PATH'] = f\"{edirect_dir}:{edirect_env['PATH']}\"\n",
    "\n",
    "        query = query.replace('\"', '\\\\\"')\n",
    "        esearch_cmd = f'esearch -db pubmed -query \"{query}\"'\n",
    "        efetch_cmd = f'efetch -format asn.1'\n",
    "\n",
    "        esearch = subprocess.Popen(shlex.split(esearch_cmd),\n",
    "                                    stdout=subprocess.PIPE, env=edirect_env)\n",
    "        out, err = esearch.communicate()\n",
    "\n",
    "        efetch = subprocess.Popen(shlex.split(efetch_cmd), stdin=subprocess.PIPE,\n",
    "                                    stdout=subprocess.PIPE, env=edirect_env)\n",
    "        out, err = efetch.communicate(input=out)\n",
    "\n",
    "        fname.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with open(fname, 'wb') as f:\n",
    "            f.write(out)\n",
    "\n",
    "        print(f'[EDirect] Finished {query}')\n",
    "\n",
    "\n",
    "    convert_xml_to_csv(subcategory, csv_dir=csv_dir, overwrite_existing=overwrite)\n",
    "\n",
    "for category in CATEGORIES:\n",
    "\n",
    "    # init folder\n",
    "    csv_dir = Path(f'data/pubmed/{category}')\n",
    "    csv_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    queries = {subc.name: subc.pubmedQuery[0]\n",
    "               for subc in ONTOLOGY[category].descendants()\n",
    "               if len(subc.pubmedQuery) > 0}\n",
    "\n",
    "    Parallel(n_jobs=5)(delayed(\n",
    "        pubmed_pipeline\n",
    "    )(subcategory, query, csv_dir=csv_dir, overwrite=True)\n",
    "                       for subcategory, query in queries.items())\n",
    "\n",
    "# clear_output()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have separate CSV files, we combine them and store the whole PubMed abstracts corpus as a single compressed CSV file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregation\n",
    "\n",
    "corpus_files = Path('data/pubmed/').glob('**/*.csv')\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for fname in tqdm(corpus_files):\n",
    "\n",
    "  # find categories from the file name\n",
    "  cats = re.findall('.*/pubmed/(.*)/(.*)\\\\.csv', str(fname))\n",
    "\n",
    "  # ignore other csv files\n",
    "  if len(cats) == 0:\n",
    "    continue\n",
    "\n",
    "  category = cats[0][0]\n",
    "  subcategory = cats[0][1]\n",
    "\n",
    "  df = pd.read_csv(fname)\n",
    "  df['category'] = category\n",
    "  df['subcategory'] = subcategory\n",
    "  dfs.append(df)\n",
    "\n",
    "# now aggregate all the data and store the compressed csv output (takes ~ 2min).\n",
    "pd.concat(dfs).to_csv(OUTPUT_PATH, index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm2021 = pd.read_csv('data/pubmed/abstracts_2021.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm2021.groupby('subcategory').size().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm2023 = pd.read_csv('data/pubmed/abstracts_2023.csv.gz', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84"
  },
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit ('py3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
