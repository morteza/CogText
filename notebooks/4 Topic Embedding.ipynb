{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Embedding\n",
    "This notebooks maps document embeddings to a shared topic space. Topic space reflects the underlying semantic structure of the documents, hence, might provide a more interpretable embedding.\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- `models/gpt3/abstracts_gpt3ada.npz` contains GPT-3 embeddings, one document per row.\n",
    "- `models/gpt3/abstracts_gpt3ada_pmids.csv` contains the PMIDs of the document embeddings in the above embedding dataset.\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- `models/gpt3/abstracts_gpt3ada_clusters.csv.gz` contains cluster assignments for each document.\n",
    "- `models/gpt3/abstracts_gpt3ada_weights.npz` contains membership weights of the documents in the topic space. Each row is a document and each column contains membership value to the corresponding topic.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "\n",
    "%reload_ext autoreload\n",
    "%reload_ext watermark\n",
    "\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from python.cogtext.datasets.pubmed import PubMedDataLoader\n",
    "from python.cogtext.topic_model import TopicModel\n",
    "\n",
    "%watermark\n",
    "%watermark -iv -p umap,hdbscan,joblib,numpy,numba,pytorch,tensorflow,python.cogtext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, make sure all the required embeddings and models are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUBMED = PubMedDataLoader(preprocessed=False, drop_low_occurred_labels=False).load()\n",
    "\n",
    "# for sbert all-MiniLM-L6-v2\n",
    "# doc_embeddings = np.load('models/sbert/abstracts_all-MiniLM-L6-v2.npz')['arr_0']\n",
    "# documents = pd.read_csv('models/sbert/abstracts_clusters.csv.gz', index_col=0)\n",
    "# umap_embeddings = np.load('models/sbert/abstracts_UMAP5d.npz')['arr_0']\n",
    "\n",
    "# GPT-3\n",
    "doc_embeddings = np.load('models/gpt3/abstracts_gpt3ada.npz')['arr_0']\n",
    "documents = pd.read_csv('models/gpt3/abstracts_gpt3ada_pmids.csv', index_col=0)\n",
    "umap_embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UMAP(min_dist=0.0, n_components=5, verbose=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #271: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 21 13:48:53 2022 Construct fuzzy simplicial set\n",
      "Fri Jan 21 13:48:55 2022 Finding Nearest Neighbors\n",
      "Fri Jan 21 13:48:55 2022 Building RP forest with 36 trees\n",
      "Fri Jan 21 13:50:11 2022 NN descent for 19 iterations\n",
      "\t 1  /  19\n",
      "\t 2  /  19\n",
      "\t 3  /  19\n",
      "\t 4  /  19\n",
      "\t 5  /  19\n",
      "\t 6  /  19\n",
      "\tStopping threshold met -- exiting after 6 iterations\n",
      "Fri Jan 21 13:51:54 2022 Finished Nearest Neighbor Search\n",
      "Fri Jan 21 13:52:04 2022 Construct embedding\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs completed: 100%| ██████████ 200/200 [06:45]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Jan 21 14:02:12 2022 Finished embedding\n",
      "[TopicModel] Reduced embedding dimensions. Now clustering...\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling hdbscan.hdbscan_._hdbscan_boruvka_kdtree...\n",
      "_hdbscan_boruvka_kdtree(array([[11.990156, ...,  4.206806],\n",
      "       ...,\n",
      "       [10.079483, ...,  5.457878]], dtype=float32), \n",
      "1, 1.0, 'euclidean', None, 40, True, False, -1)\n",
      "__________________________________________hdbscan_boruvka_kdtree - 21.5s, 0.4min\n",
      "[TopicModel] Clustered embeddings. Now computing weights...\n",
      "[TopicModel] Done!\n"
     ]
    }
   ],
   "source": [
    "# now run the topic modeling on the embeddings. It will take a while (~ 80min on GPT-3 Ada embeddings)\n",
    "\n",
    "model = TopicModel(parametric_umap=False, verbose=True)\n",
    "clusters, weights = model.fit_transform(doc_embeddings, umap_embeddings=umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped 382855 documents to a 509-dimensional topic space, while marking 172957 documents as noise.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# report number of noise documents\n",
    "n_noise_documents = documents['cluster'].isna().sum()\n",
    "\n",
    "print(f'Mapped {doc_embeddings.shape[0]} documents to a {weights.shape[1]}-dimensional topic space, '\n",
    "      f'while marking {n_noise_documents} documents as noise.')\n",
    "\n",
    "# # drop cluster \"-1\" and make the rest 1-indexed\n",
    "documents['cluster'] = np.where(clusters >= 0, clusters + 1, np.nan)\n",
    "\n",
    "# # store\n",
    "documents.to_csv('models/gpt3/abstracts_gpt3ada_clusters.csv.gz', index=True)\n",
    "np.savez('models/gpt3/abstracts_gpt3ada_weights.npz', weights)\n",
    "\n",
    "# DEBUG: report cluster frequencies\n",
    "# documents['cluster'].value_counts()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d4c55ad0dd25f9ca95e4d49a929aa3f71bfb37020ae570a9996c3e164818202"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('py3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
