{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Topic Embedding\n",
    "\n",
    "This notebook implements an analysis that first produces a topic embedding for cognitive tasks and constructs. Topic embedding refers to the probabilities of assigning a topic to a given task/construct corpus. For example, task A could be assigned the following topic embedding: `[1., .5, .1]` which basically shows the probability of observing the three topics in the corpus A.\n",
    "\n",
    "## Data\n",
    "\n",
    "**Input**: `pubmed_abstracts.csv.gz`.\n",
    "\n",
    "**Output**: `topic_embeddings` is a table in that each row denotes a document, columns are topics, and values are the probabilities of being assigned to topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from bertopic import BERTopic\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute '_rebuild_function' on <module 'numba.core.serialize' from '/Users/morteza/miniconda3/envs/py3/lib/python3.9/site-packages/numba/core/serialize.py'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/3_/gmvd1nkx285133z5yh3chz2c0000gp/T/ipykernel_98027/2305058396.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'models/{model_name}_{version}.idx.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mBERTopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'models/{model_name}_{version}.model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mtopics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'models/{model_name}_{version}.topics.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'models/{model_name}_{version}.probs.npz'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'arr_0'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.9/site-packages/bertopic/_bertopic.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(cls, path, embedding_model)\u001b[0m\n\u001b[1;32m   1283\u001b[0m                 \u001b[0mtopic_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1284\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1285\u001b[0;31m                 \u001b[0mtopic_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1286\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtopic_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.9/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'name'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0m_read_fileobject\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m             \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.9/site-packages/joblib/numpy_pickle.py\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.9/pickle.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1210\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mEOFError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1211\u001b[0m                 \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1212\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1213\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.9/pickle.py\u001b[0m in \u001b[0;36mload_reduce\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1587\u001b[0m         \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1588\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1589\u001b[0;31m         \u001b[0mstack\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1590\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mREDUCE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_reduce\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py3/lib/python3.9/site-packages/numba/core/serialize.py\u001b[0m in \u001b[0;36m_unpickle__CustomPickled\u001b[0;34m(serialized)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mUses\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mNumbaPickler\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mto\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \"\"\"\n\u001b[0;32m---> 97\u001b[0;31m     \u001b[0mctor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserialized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_CustomPickled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can't get attribute '_rebuild_function' on <module 'numba.core.serialize' from '/Users/morteza/miniconda3/envs/py3/lib/python3.9/site-packages/numba/core/serialize.py'>"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "pubmed_path = Path('data/pubmed_abstracts_preprocessed.csv.gz')\n",
    "model_name = 'pubmed100pct_bertopic'\n",
    "version = 'v202110141'\n",
    "\n",
    "indices = np.load(f'models/{model_name}_{version}.idx.npz')['arr_0']\n",
    "model: BERTopic = BERTopic.load(f'models/{model_name}_{version}.model')\n",
    "topics = np.load(f'models/{model_name}_{version}.topics.npz')['arr_0']\n",
    "probs = np.load(f'models/{model_name}_{version}.probs.npz')['arr_0']\n",
    "\n",
    "data = pd.read_csv(pubmed_path)\n",
    "data = data[data.index.isin(indices)]\n",
    "\n",
    "# DEBUG\n",
    "# with pd.option_context('display.max_rows', 10000):\n",
    "#   display(model.get_topic_info())\n",
    "\n",
    "# model.visualize_topics()\n",
    "\n",
    "# model.visualize_barchart()\n",
    "# model.visualize_distribution(probabilities=probs[0])\n",
    "\n",
    "# model.get_params()\n",
    "# model.get_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before train: 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:01<00:00,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after train: 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "n_topics = 300\n",
    "batch_size = 200\n",
    "n_samples = 10000\n",
    "n_labels = 2\n",
    "\n",
    "X, y = make_classification(n_samples=n_samples, n_features=n_topics, n_classes=n_labels, random_state=0)\n",
    "\n",
    "X = torch.tensor(X).type(torch.float)\n",
    "y = torch.tensor(y).type(torch.long)\n",
    "\n",
    "class TopicEmbeddingNet(nn.Module):\n",
    "  def __init__(self, n_topics: int, n_labels: int):\n",
    "    super(TopicEmbeddingNet, self).__init__()\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "        nn.Linear(n_topics, n_topics), nn.BatchNorm1d(196), nn.LeakyReLU(0.1),\n",
    "        nn.Linear(196, 128), nn.BatchNorm1d(128), nn.LeakyReLU(0.1),\n",
    "        nn.Linear(128, n_topics)\n",
    "    )\n",
    "    self.hidden2mu = nn.Linear(n_topics, n_topics)\n",
    "    self.hidden2log_var = nn.Linear(n_topics, n_topics)\n",
    "    self.decoder = nn.Sequential(\n",
    "      nn.Linear(n_topics, 128), nn.BatchNorm1d(128), nn.LeakyReLU(0.1),\n",
    "      nn.Linear(128, 196), nn.BatchNorm1d(196), nn.LeakyReLU(0.1),\n",
    "      nn.Linear(196, n_topics),\n",
    "    )\n",
    "\n",
    "    self.encoder = nn.Sequential(\n",
    "      nn.Linear(n_topics, n_topics), nn.BatchNorm1d(196), nn.LeakyReLU(0.1)\n",
    "    )\n",
    "    self.embedding = nn.Embedding(n_labels, n_topics)\n",
    "\n",
    "  def encode(self, x, label):\n",
    "    # h = self.encoder(x)\n",
    "    # mu = self.hidden2mu(h)\n",
    "    # log_var = self.hidden2log_var(h)\n",
    "    # sigma = torch.exp(0.5*log_var)\n",
    "    # z = torch.randn_like(sigma)\n",
    "    # h = mu + sigma * z\n",
    "    # return mu, log_var, h\n",
    "\n",
    "    x = self.encoder(x)\n",
    "    h = self.embedding(label)\n",
    "\n",
    "  def decode(self, x):\n",
    "    x = self.decoder(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    mu, log_var, h = self.encode(x)\n",
    "    y = self.decoder(h)\n",
    "    return mu, log_var, y\n",
    "\n",
    "model = TopicEmbeddingNet(n_topics, n_labels)\n",
    "\n",
    "print('before train:', (model(X)[2].argmax(dim=1) == y).sum().item())\n",
    "\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "for epoch in tqdm(range(3)):\n",
    "  model.train()\n",
    "  model.zero_grad()\n",
    "  _, _, x_pred = model(X)\n",
    "  loss = criterion(x_pred, X)\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n",
    "  # TODO eval\n",
    "\n",
    "print('after train:', (model(X)[2].argmax(dim=1) == y).sum().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([300, 300])\n"
     ]
    }
   ],
   "source": [
    "print(dict(model.named_parameters())['hidden2mu.weight'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import skorch\n",
    "from skorch.callbacks import TensorBoard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "cols = ['category', 'subcategory']\n",
    "X = np.vstack([data[col].astype('category').cat.codes for col in cols]).T\n",
    "y = probs\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.int)\n",
    "y = torch.tensor(y, dtype=torch.float)\n",
    "\n",
    "\n",
    "net = skorch.NeuralNetRegressor(\n",
    "  TopicEmbeddingNet(1,1,1),\n",
    "  max_epochs=100,\n",
    "  lr=0.1,\n",
    "  iterator_train__shuffle=True,\n",
    "  # DEBUG: callbacks=[TensorBoard(writer=SummaryWriter())]\n",
    ")\n",
    "\n",
    "# DEBUG: net.fit(X, y)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "  'lr': [0.01],\n",
    "  'max_epochs': [100],\n",
    "  'module__n_cats': [data['category'].nunique()],\n",
    "  'module__n_subcats': [data['subcategory'].nunique()],\n",
    "  'module__embeddings_dim': range(1, y.shape[1]),\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(net, params, scoring='accuracy')\n",
    "\n",
    "gs.fit(X, y)\n",
    "print(gs.best_score_, gs.best_params_, gs.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'skorch.regressor.NeuralNetRegressor'>[uninitialized](\n",
       "  module=TopicEmbeddingNet(\n",
       "    (cat_embedding): Embedding(1, 1)\n",
       "    (subcat_embedding): Embedding(1, 1)\n",
       "    (fc): Linear(in_features=1, out_features=65, bias=True)\n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = gs.best_params_\n",
    "\n",
    "gs.estimator\n",
    "\n",
    "# with torch.no_grad():\n",
    "#   params = list(model.get_params()['module'].parameters())\n",
    "#   cat_embeddings = params[0]\n",
    "#   subcat_embeddings = params[1]\n",
    "\n",
    "# model.save_params(f_params='model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test/train RSA\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "sim_train = cosine_similarity(model.topic_embeddings)\n",
    "sim_test = cosine_similarity(result.H_test)\n",
    "rho = spearmanr(sim_train, sim_test)\n",
    "print(f'[RSA] mean test/train correlation: {rho[0].mean():.2f}')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "266722041ed6426a0a88c0d75e9dd39659f44e3a6fea07300cd13bea36eb387d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('py3': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
