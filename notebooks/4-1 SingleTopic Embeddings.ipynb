{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SingleTopic Embeddings\n",
    "\n",
    "The assumption that a document belongs only to one topic simplifies the model without losing generalization.\n",
    "\n",
    "After assigning documents to the topics, the 8semantic embeddings* of the documents can be calculated by the matrix $H$ where element $_H{ij}$ shows the probability of assigning document $i$ to the topic $j$. For brevity, this embedding assumes each document is assigned to only one topic:\n",
    "\n",
    "$$\\overrightarrow Z_{i} = \\argmax(\\overrightarrow H_{i})$$\n",
    "\n",
    "$$|\\overrightarrow{Z}| = |\\overrightarrow{H}| = N_{\\text{topics}}$$\n",
    "\n",
    "where $Z_i$ is a one-hot vector where the only non-zero value at index $j$ shows the probability of assigning the document $i$ to the topic $j$. $H_{i}$ is the topics membership vector for the document $i$ as generated by HDBSCAN during the topic modeling; element $H_{ij}$ shows the probability of assigning document $i$ to the topic $j$, and $\\sum_{j} H_{ij} = 1$ holds.\n",
    "\n",
    "Consequently, the topic embeddings of a document set, $L_j$, can be calculated by pooling the embeddings of its member documents.\n",
    "\n",
    "$$\\overrightarrow Z_{L_j} = \\sum_{i \\in L_j} \\overrightarrow  Z_{i}$$\n",
    "\n",
    "$$|\\overrightarrow{Z}| = N_{\\text{topics}}$$\n",
    "\n",
    "where $L_j$ is a list of document indices, and $Z_{L_j}$ is the embedding for the label $j$ in the topics space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requirements\n",
    "%pip install -Uq bertopic matplotlib seaborn xmltodict\n",
    "%pip install -Uq git+https://github.com/scikit-learn-contrib/hdbscan\n",
    "\n",
    "# Creating a new conda env is highly recommended because of the conflicting packages.\n",
    "# %conda activate bertopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()  # noqa\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from python.cogtext.datasets.pubmed import PubMedDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = PubMedDataLoader()\n",
    "data = data_loader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data, test_data = train_test_split(data,\n",
    "                                         test_size=0.5,\n",
    "                                         stratify=data['label'],\n",
    "                                         random_state=42)\n",
    "\n",
    "X_train = train_data['abstract'].values\n",
    "y_train = train_data['label'].astype('category').cat.codes\n",
    "\n",
    "X_test = test_data['abstract'].values\n",
    "y_test = test_data['label'].astype('category').cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# pretrained document embeddings\n",
    "embeddings_file = 'models/universal-sentence-encoder-v4/abstracts_embeddings.npz'\n",
    "# embeddings_file = 'models/all-MiniLM-L6-v2/abstracts_embeddings.npz'\n",
    "\n",
    "doc_embedding_model = SentenceTransformer('all-distilroberta-v1')\n",
    "\n",
    "doc_embeddings = np.load(embeddings_file)['arr_0']\n",
    "train_doc_embeddings = doc_embeddings[train_data.index]\n",
    "test_doc_embeddings = doc_embeddings[test_data.index]\n",
    "\n",
    "# OR retrain the document embedding model from scratch\n",
    "# doc_embeddings = doc_embedding_model.encode(X_train, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# UMAP\n",
    "umap_model = UMAP(n_neighbors=15,\n",
    "                  n_components=5,\n",
    "                  min_dist=0.0,\n",
    "                  metric='cosine',\n",
    "                  low_memory=False)\n",
    "\n",
    "# HDBSCAN\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=5,\n",
    "                        metric='euclidean',\n",
    "                        prediction_data=True)\n",
    "\n",
    "vectorizer_model = CountVectorizer(ngram_range=(1, 4),\n",
    "                                   stop_words='english',\n",
    "                                  #  max_features=20000,\n",
    "                                   max_df=int(X_train.shape[0] * 1.0),\n",
    "                                   min_df=5)\n",
    "\n",
    "# BERTopic\n",
    "model = BERTopic(calculate_probabilities=False,\n",
    "                 nr_topics='auto',\n",
    "                 embedding_model=doc_embedding_model,\n",
    "                 umap_model=umap_model,\n",
    "                 hdbscan_model=hdbscan_model,\n",
    "                 vectorizer_model=vectorizer_model,\n",
    "                 verbose=True)\n",
    "\n",
    "# fit the topic model\n",
    "train_topics, train_scores = model.fit_transform(documents=X_train, y=y_train,\n",
    "                                                 embeddings=train_doc_embeddings)\n",
    "\n",
    "test_topics, test_scores = model.transform(documents=X_test,\n",
    "                                           embeddings=test_doc_embeddings)\n",
    "\n",
    "model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.visualize_hierarchy()\n",
    "# model.visualize_term_rank()\n",
    "# model.visualize_topics()\n",
    "# model.visualize_barchart()\n",
    "\n",
    "sns.displot(train_scores,)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data['topic'] = train_topics\n",
    "# train_data['topic_score'] = train_scores\n",
    "# train_data['topic'].replace({-1:np.nan}, inplace=True)\n",
    "# train_data.query('topic.isna()')['topic_score'] = 0.0\n",
    "# Z = train_data[['topic','topic_score','label']]\n",
    "\n",
    "# Z.groupby('label').mean()\n",
    "\n",
    "# pd.get_dummies(train_data['topic']).iloc[10].sum()\n",
    "\n",
    "n_topics = np.unique(train_topics + test_topics).shape[0]\n",
    "Z_train = np.zeros((train_data.shape[0], n_topics-1))\n",
    "Z_test = np.zeros((test_data.shape[0], n_topics-1))\n",
    "\n",
    "for i, (topic, score) in enumerate(zip(train_topics, train_scores)):\n",
    "  if topic != -1:\n",
    "    Z_train[i,topic] = score\n",
    "\n",
    "for i, topic in enumerate(test_topics):\n",
    "  if topic != -1:\n",
    "    Z_test[i,topic] = 1.0\n",
    "\n",
    "# confirming that non-zero element is valid and that one-hot encoding works as expected\n",
    "assert all(Z_train.sum(axis=1) == np.array(train_scores))\n",
    "assert all((Z_train.argmax(axis=1) == train_topics) | (np.array(train_topics) == -1))\n",
    "\n",
    "# assert all(Z_test.sum(axis=1) == np.array(test_scores))\n",
    "assert all((Z_test.argmax(axis=1) == test_topics) | (np.array(test_topics) == -1))\n",
    "\n",
    "Z_train = pd.DataFrame(Z_train)\n",
    "Z_train['label'] = train_data['label']\n",
    "Z_train = Z_train.groupby('label').sum()\n",
    "\n",
    "Z_test = pd.DataFrame(Z_test)\n",
    "Z_test = Z_test.iloc[:,:100]\n",
    "Z_test['label'] = train_data['label']\n",
    "Z_test = Z_test.groupby('label').sum()\n",
    "\n",
    "# Z_docs.corr('pearson')\n",
    "# sns.clustermap(Z_test, standard_scale=1)\n",
    "# Z_test.corr('pearson')\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "Z_sim = pd.DataFrame(cosine_similarity(Z_test), index=Z_test.index, columns=Z_test.index)\n",
    "\n",
    "categories = Z_sim.index.to_series().apply(lambda x: test_data.query('label == @x').iloc[0]['category'])\n",
    "pallette = ['darkgreen', 'gold']\n",
    "colors = [pallette[c] for c in categories.astype('category').cat.codes.to_list()]\n",
    "\n",
    "sns.clustermap(Z_sim, figsize=(25,25), cmap='RdBu', row_colors=colors)\n",
    "plt.show()\n",
    "\n",
    "# Z_test_normalized = Z_test.div(Z_test.sum(axis=1), axis=0).fillna(0.0)\n",
    "# sns.clustermap(Z_test_normalized, col_cluster=False, figsize=(25,25))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4d4c55ad0dd25f9ca95e4d49a929aa3f71bfb37020ae570a9996c3e164818202"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('py3': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
