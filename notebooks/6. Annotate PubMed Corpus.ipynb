{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd05ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84",
   "display_name": "Python 3.9.4 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Annotating cognitive tasks corpus\n",
    "\n",
    "This notebooks implements a graphical interface to annotate PubMed cognitive tests coprpus and mark each article as relevant or irrelated to the provided context.\n",
    "\n",
    "First, run all the cells up to the last one, which saves the annotated outputs into the `data/pubmed/tests_annotated.csv` file. You don't need to annotate all the articles at once. Whenever you run the notebook, it only shows those articles that are not already annotated."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pigeon import annotate\n",
    "import pigeonXT as pixt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display"
   ]
  },
  {
   "source": [
    "## Load corpus\n",
    "\n",
    "The following loads all the CSV files from the `data/pubmed/tests/` directory and prepares them to be annotated."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Reading CSV files: 91it [00:05, 15.57it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if not Path('data/pubmed/annotated_cognitive_tests_corpus.csv').exists():\n",
    "    csv_files = Path('data/pubmed').glob('tests/*.csv')\n",
    "\n",
    "    corpora = []\n",
    "    for csv_file in tqdm(csv_files, desc='Reading CSV files'):\n",
    "        df = pd.read_csv(csv_file)\n",
    "        df['corpus_name'] = csv_file.stem\n",
    "        corpora.append(df)\n",
    "\n",
    "    df = pd.concat(corpora, axis=0)\n",
    "    df['abstract'].fillna(df['title'], inplace=True)\n",
    "    df['is_annotated'] = False\n",
    "    df['annotation'] = None\n",
    "    df.to_csv('data/pubmed/annotated_cognitive_tests_corpus.csv', index=False, na_rep='n/a')\n",
    "else:\n",
    "    # load the aggregated corpus if the aggregated file already exists\n",
    "    df = pd.read_csv('data/pubmed/annotated_cognitive_tests_corpus.csv')\n",
    "\n",
    "# workaround to discard unusual terminators in the text\n",
    "df['abstract'] = df['abstract'].apply(lambda x: x.replace('\\u2029', ' ') if isinstance(x, str) else x)\n",
    "df['title'] = df['title'].apply(lambda x: x.replace('\\u2029', ' ') if isinstance(x, str) else x)"
   ]
  },
  {
   "source": [
    "##  Annonate PubMed corpus\n",
    "\n",
    "For each article, you will be asked to annotate them as `relevant` or `irrelevant`. You need to make sure of the following constraints for each question:\n",
    "\n",
    "- `journal_title` refers to a cogntive science journal.\n",
    "- `title` and `abstract` are in theory and practice related to the context that is provided in the `corpus_name`.\n",
    "- and all the other features make sense.\n",
    "\n",
    "You will have three choices: relevant, irrelevant, and skip.\n",
    "\n",
    "When you are done with annotating, just run the next cell to store your work. Don't worry; next time you run this annotating process, you will only see those articles that are not previously annotated."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HTML(value='0 of 91229 Examples annotated, Current Position: 0 ')",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "49c66c19690f4095bb4a7839d7b9057d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "HBox(children=(Button(description='relevant', style=ButtonStyle()), Button(description='irrelevant', style=Butâ€¦",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "877b957b09c94087a9e8e9dd27e33cfa"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a4b2875a54484d00a2edd1745b261f90"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "def store_annotations(annotations, path = 'data/pubmed/annotated_cognitive_tests_corpus.csv'):\n",
    "  df.loc[list(annotations['row_index']), 'is_annotated'] = annotations['changed']\n",
    "  df.loc[list(annotations['row_index']), 'annotation'] = annotations['annotation']\n",
    "  df.to_csv(path, index=False, na_rep='n/a')\n",
    "\n",
    "\n",
    "annotations = pixt.annotate(\n",
    "  df.query('is_annotated.isna() or (is_annotated == False)').index.to_list(),\n",
    "  options=['relevant','irrelevant'],\n",
    "  # task_type='multilabel-classification',\n",
    "  # shuffle=True,\n",
    "  buttons_in_a_row=5,\n",
    "  example_column='row_index',\n",
    "  value_column='annotation',\n",
    "  display_fn=lambda x: display(x,df[x:x+1].T),\n",
    "  final_process_fn=lambda annotations: store_annotations(annotations)\n",
    ")"
   ]
  },
  {
   "source": [
    "## Store the newly annonated articles\n",
    "\n",
    "**do NOT run this cell except when you're done annotating and want to save the results.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_annotations(annotations)"
   ]
  }
 ]
}