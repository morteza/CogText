{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.4 64-bit ('py3': conda)"
  },
  "interpreter": {
   "hash": "5ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# A Task-Construct space using an IRT-like factorization\n",
    "\n",
    "We are going to develop an latent space shared between tasks and constructs, given a non-negative probability matrix, X, in which rows are tasks, columns are constructs, and cell values show the probability that respective task and construct are used simultaniously in the same PubMed publications.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "source": [
    "## Load and prepare matrix X"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "df = pd.read_csv('data/pubmed/test_construct_matrix.csv')\n",
    "\n",
    "# X = np.random.random((10,10))\n",
    "X = df.pivot(index='test',columns='construct',values='shared_corpus_size')"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "source": [
    "## Matrix Factorization\n",
    "\n",
    "There are a variety of methods to factorize a matrix in recommender systems literature, including methods to fill up missing values or approximate using neural networks. In our case, we get access to the full probability matrix, so we don't need those kinds of sophisticated techniques and simple factorization like Non-Negative Factorization (NMF) would suffice. We hence only optimize the hyper-parameter (`n_components`) by minimizing the MSE cost."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NMF(n_components=10, max_iter=1000000, init='random')\n",
    "\n",
    "W = model.fit_transform(X)\n",
    "H = model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(6810, 6725, 6725)"
      ]
     },
     "metadata": {},
     "execution_count": 113
    }
   ],
   "source": [
    "# MSE error\n",
    "((W@H) - X).values.argmax(), (W@H).argmax(), X.values.argmax()\n",
    "# X.mean()"
   ]
  },
  {
   "source": [
    "## Visualize embeddings\n",
    "\n",
    "TODO"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}