{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd05ddcf14c786c671500c086f61f0b66d0417d6c58ff12753e346e191a84f72b84",
   "display_name": "Python 3.9.4 64-bit ('py3': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import OWL, Graph\n",
    "from rdflib.namespace import RDFS\n",
    "from owlready2 import get_ontology, default_world\n",
    "import xmltodict\n",
    "import time\n",
    "from rdflib import URIRef\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "# one liner to import cogtext package from `../python` folder.\n",
    "#if '../python' not in sys.path: sys.path.append('../python');\n",
    "from python.cogtext import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl_file = 'data/ontologies/efo.owl'\n",
    "owl_prefix = 'http://www.semanticweb.org/morteza/ontologies/2019/11/executive-functions-ontology#'\n",
    "\n",
    "ONTOLOGY = get_ontology(owl_file).load()\n",
    "GRAPH = default_world.as_rdflib_graph()\n",
    "\n",
    "def query(graph, parent_cls='Task'):\n",
    "    \"\"\"Function to query tasks, constructs, regions, etc.\n",
    "\n",
    "    ## Returns\n",
    "    A list of labels\n",
    "    \"\"\"\n",
    "\n",
    "    cls_name = parent_cls[1:] if parent_cls.startswith(\":\") else parent_cls\n",
    "\n",
    "    sparql_query = f\"\"\"\n",
    "    prefix : <{owl_prefix}>\n",
    "\n",
    "    SELECT ?label ?pubmed_query\n",
    "    WHERE {{\n",
    "    ?cls rdfs:subClassOf* :{cls_name};\n",
    "          :pubmedQuery ?pubmed_query;\n",
    "          rdfs:label ?label\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    print(sparql_query)\n",
    "    # select the all rdfs:labels, flatten the list of labels, and convert them to python string\n",
    "    labels = [labels for labels in graph.query(sparql_query)]\n",
    "    pubmed_queries = {l[0].toPython(): l[1].toPython() for l in labels}\n",
    "    return pubmed_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n    prefix : <http://www.semanticweb.org/morteza/ontologies/2019/11/executive-functions-ontology#>\n\n    SELECT ?label ?pubmed_query\n    WHERE {\n    ?cls rdfs:subClassOf* :CognitiveProcess;\n          :pubmedQuery ?pubmed_query;\n          rdfs:label ?label\n    }\n    \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'Focused Attention': '(\"Focused Attention\" AND \"Cognitive\"[TIAB])',\n",
       " 'Selective Attention': '(\"Selective Attention\" AND \"Cognitive\"[TIAB])',\n",
       " 'Behavioral Control': '(\"Behavioral Control\" and \"Cognitive\"[TIAB])',\n",
       " 'Behavioral Regulation': '(\"Behavioral Regulation\" and \"Cognitive\"[TIAB])',\n",
       " 'Central Executive': '(\"Central Executive\" and \"Cognitive\"[TIAB])',\n",
       " 'Cognitive Flexibility': '(\"Cognitive Flexibility\"[TIAB])',\n",
       " 'Cognitive Control': '(\"Cognitive Control\"[TIAB])',\n",
       " 'Context Processing': '(\"Context Processing\" and \"Cognitive\"[TIAB])',\n",
       " 'Core Executive Functions': '(Core Executive Functions[TIAB])',\n",
       " 'Inhibition': '(\"Inhibition\" AND \"Cognitive\"[TIAB])',\n",
       " 'Inhibitory Control': '(\"Inhibitory Control\" AND \"Cognitive\"[TIAB])',\n",
       " 'Interference Control': '(\"Interference Control\" AND \"Cognitive\"[TIAB])',\n",
       " 'Cognitive Inhibition': '(\"Cognitive Inhibition\"[TIAB])',\n",
       " 'Response Inhibition': '(\"Response Inhibition\" AND \"Cognitive\"[TIAB])',\n",
       " 'Working Memory': '(\"Working Memory\" AND \"Cognitive\"[TIAB])',\n",
       " 'Visual-Spatial Working Memory': '(\"Visual Spatial Working Memory\"[TIAB])',\n",
       " 'Verbal Working Memory': '(\"Verbal Working Memory\" AND \"Cognitive\"[TIAB])',\n",
       " 'Creativity': '(\"Creativity\" AND \"Cognitive\"[TIAB])',\n",
       " 'Deductive Reasoning': '(\"Deductive Reasoning\" AND \"Cognitive\"[TIAB])',\n",
       " 'Dual Task Coordination': '(\"Dual Task Coordination\" AND \"Cognitive\"[TIAB])',\n",
       " 'Effective Performance': '(\"Effective Performance\" AND \"Cognitive\"[TIAB])',\n",
       " 'Effortful Control': '(\"Effortful Control\" AND \"Cognitive\"[TIAB])',\n",
       " 'Emotional Control': '(\"Emotional Control\" AND \"Cognitive\"[TIAB])',\n",
       " 'Emotional Regulation': '(\"Emotional Regulation\" AND \"Cognitive\"[TIAB])',\n",
       " 'Executive Control': '(\"Executive Control\" AND \"Cognitive\"[TIAB])',\n",
       " 'Feedback Learning': '(\"Feedback Learning\" AND \"Cognitive\"[TIAB])',\n",
       " 'Fluency': '(\"Fluency\" AND \"Cognitive\"[TIAB])',\n",
       " 'Generativity': '(\"Generativity\" AND \"Cognitive\"[TIAB])',\n",
       " 'Higher Order Executive Function': '(Higher Order Executive Functions[TIAB])',\n",
       " 'Problem Solving': '(\"Problem Solving\" AND \"Cognitive\"[TIAB])',\n",
       " 'Logical Reasoning': '(\"Logical Reasoning\" AND \"Cognitive\"[TIAB])',\n",
       " 'Relational Reasoning': '(\"Relational Reasoning\" AND \"Cognitive\"[TIAB])',\n",
       " 'Initiation': '(\"Initiation\" AND \"Cognitive\"[TIAB])',\n",
       " 'Mindfulness': '(\"Mindfulness\" AND \"Cognitive\"[TIAB])',\n",
       " 'Motivational Drive': '(\"Motivational Drive\"[TIAB])',\n",
       " 'Organization': '(\"Organization\" AND \"Cognitive\"[TIAB])',\n",
       " 'Responsive Action': '(\"Responsive Action\" AND \"Cognitive\"[TIAB])',\n",
       " 'Response Conflict': '(\"Response Conflict\" AND \"Cognitive\"[TIAB])',\n",
       " 'Reward Processing': '(\"Reward Processing\" AND \"Cognitive\"[TIAB])',\n",
       " 'Self-Control': '(\"Self Control\" AND \"Cognitive\"[TIAB])',\n",
       " 'Self Monitoring': '(\"Self Monitoring\" AND \"Cognitive\"[TIAB])',\n",
       " 'Self-Regulation': '(\"Self Regulation\" AND \"Cognitive\"[TIAB])',\n",
       " 'Sequencing': '(\"Sequencing\" AND \"Cognitive\"[TIAB])',\n",
       " 'Set Shifting': '(\"Shifting\" AND \"Cognitive\"[TIAB])',\n",
       " 'Strategic Retrieval': '(\"Strategic Retrieval\" AND \"Cognitive\"[TIAB])',\n",
       " 'Switching': '(\"Task Switching\" AND \"Cognitive\"[TIAB])',\n",
       " 'Task Switching': '(\"Task Switching\" AND \"Cognitive\"[TIAB])',\n",
       " 'Updating': '(\"Updating\" AND \"Cognitive\"[TIAB])',\n",
       " 'Verbal Processing': '(\"Verbal Processing\" AND \"Cognitive\"[TIAB])',\n",
       " 'Visuospatial Processing': '(\"Visuospatial Processing\" AND \"Cognitive\"[TIAB])',\n",
       " 'Volition': '(\"Volition\" AND \"Cognitive\"[TIAB])'}"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# pubmed_queries = query(GRAPH, 'BaggettaTask')   # dictionary of task_name -> pubmed_query\n",
    "# print(f'{len(pubmed_queries)} tasks found in the EF ontology (baggetta2016)')\n",
    "\n",
    "pubmed_queries = query(GRAPH, 'CognitiveProcess')   # dictionary of task_name -> pubmed_query\n",
    "pubmed_queries\n"
   ]
  },
  {
   "source": [
    "# search and cache hits\n",
    "\n",
    "Let's search PubMed for each task query (`ef:pubmedQuery`), then cleanup the XML results into a CSV with the following columns: abstract, title, and a reference to the metadata (pmid).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PubMed] (Degraded Vigilance task* \"PVT\"[TIAB])\n",
      "[PubMed] Succesfully stored 4 hits on NCBI history server.\n",
      "[PubMed] Succesfully stored hits in ../data/pubmed/.cache/Degraded Vigilance.xml.\n",
      "[PubMed] (\"Delay Choice\" Task*[TIAB])\n",
      "[PubMed] Succesfully stored 15 hits on NCBI history server.\n",
      "[PubMed] Succesfully stored hits in ../data/pubmed/.cache/Delay Choice.xml.\n",
      "[PubMed] (\"Dimensional Change Card\" Sort*[TIAB])\n",
      "[PubMed] Succesfully stored 134 hits on NCBI history server.\n",
      "[PubMed] Succesfully stored hits in ../data/pubmed/.cache/DCCS (Dimentional Change Card Sort).xml.\n",
      "[XML2CSV] converting \"Degraded Vigilance\" dataset...\n",
      "[XML2CSV] converting \"Delay Choice\" dataset...\n",
      "[XML2CSV] converting \"DCCS (Dimentional Change Card Sort)\" dataset...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for task_label, pubmed_query in pubmed_queries.items():\n",
    "    task_label = task_label.replace('/','') \n",
    "    fname = Path('../data/pubmed/.cache') / (task_label + '.xml')\n",
    "\n",
    "    if not fname.exists():\n",
    "        search_and_store(pubmed_query, fname)\n",
    "\n",
    "\n",
    "def find_mesh(mesh_list):\n",
    "    \"\"\"Extracts MeSH names from a list of XML MedlineCitation.MeshHeadingList.MeshHeading tags.\"\"\"\n",
    "    if not isinstance(mesh_list, list):\n",
    "        return []\n",
    "\n",
    "    mesh_names = [h['DescriptorName']['#text'] for h in mesh_list]# if d['DescriptorName']['@MajorTopicYN'] == 'Y']\n",
    "    return mesh_names\n",
    "\n",
    "\n",
    "# now cleanup and convert all abstracts into CSV files\n",
    "for task_label in pubmed_queries.keys():\n",
    "\n",
    "    task_fname = task_label.replace('/','')\n",
    "\n",
    "    xml_file = Path('../data/pubmed/.cache') / (task_fname + '.xml')\n",
    "    csv_file = Path('../data/pubmed/tests') / (task_fname + '.csv')\n",
    "\n",
    "    if not csv_file.exists():\n",
    "        with open(xml_file, 'r') as f:\n",
    "            xml_content = xmltodict.parse(f.read())\n",
    "            if 'PubmedArticleSet' in xml_content:\n",
    "                print(f'[XML2CSV] converting \"{task_label}\" dataset...')\n",
    "\n",
    "                df = pd.json_normalize(xml_content['PubmedArticleSet']['PubmedArticle'])\n",
    "\n",
    "                # pmid, title, and abstract\n",
    "                df['pmid'] = df['MedlineCitation.PMID.#text']\n",
    "                df['title'] = df['MedlineCitation.Article.ArticleTitle']\n",
    "                df['abstract'] = df['MedlineCitation.Article.Abstract.AbstractText'].apply(cleanup_abstract)\n",
    "                \n",
    "                # publication year\n",
    "                df['year'] = df['MedlineCitation.Article.Journal.JournalIssue.PubDate.Year']\n",
    "                df['journal_title'] = df['MedlineCitation.Article.Journal.Title']\n",
    "                df['journal_iso_abbreviation'] = df['MedlineCitation.Article.Journal.ISOAbbreviation']\n",
    "\n",
    "                # MeSh topics (some datasets do not contain MeshHeading, e.g., Spin The Pots)\n",
    "                # if 'MedlineCitation.MeshHeadingList.MeshHeading' in df.columns:\n",
    "                #     df['mesh'] = df['MedlineCitation.MeshHeadingList.MeshHeading'].apply(find_mesh)\n",
    "                # else:\n",
    "                #     df['mesh'] = None\n",
    "\n",
    "                if 'MedlineCitation.Article.Journal.JournalIssue.PubDate.MedlineDate' in df.columns:\n",
    "                    medline_year = df['MedlineCitation.Article.Journal.JournalIssue.PubDate.MedlineDate'].apply(lambda x: x[0:4] if isinstance(x, str) and len(x)>=4 else x)\n",
    "                    df['year'].fillna(medline_year, inplace=True)\n",
    "\n",
    "                # fill missing abstracts with #text value\n",
    "                if 'MedlineCitation.Article.Abstract.AbstractText.#text' in df.columns:\n",
    "                    df['abstract'].fillna(df['MedlineCitation.Article.Abstract.AbstractText.#text'], inplace=True)\n",
    "\n",
    "                if 'MedlineCitation.Article.ArticleTitle.#text' in df.columns:\n",
    "                    df['title'].fillna(df['MedlineCitation.Article.ArticleTitle.#text'], inplace=True)\n",
    "\n",
    "                # workaround to discard unusual terminators in the abstracts\n",
    "                df['abstract'] = df['abstract'].apply(lambda x: x.replace('\\u2029', ' ') if isinstance(x, str) else x)\n",
    "\n",
    "                df[['pmid', 'year', 'journal_title', 'journal_iso_abbreviation', 'title','abstract']].to_csv(csv_file, index=False)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "source": [
    "## Data cleansing\n",
    "\n",
    "Here, we aim to preprocess PubMed corpora and keep only those relevant metadata. Outputs of this pipeline are stored in the `data/pubmed` folder as csv files; one csv per task corpus.\n",
    "\n",
    "The following metadata will be stored in the processed csv files:\n",
    "\n",
    "- pmid: unique PubMed identifier of the article.\n",
    "- title: escaped title in string format.\n",
    "- journal_title: Journal title\n",
    "- journal_iso_abbreviation: Journal ISO abbreviation\n",
    "- abstract: escaped and cleanedup abstract in string format.\n",
    "- year: publication year in YYYY format.\n",
    "- mesh: A list of Medical Subject Headings which contains the field of research and other topics. We only keep major topics."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}