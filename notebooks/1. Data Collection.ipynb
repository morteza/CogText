{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd0ea20dcc4936bc3e336221d5e5ae7bcd09719da9b70640ea666426cf2d919dc3d",
   "display_name": "Python 3.9.2 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "from rdflib import OWL, Graph\n",
    "from rdflib.namespace import RDFS\n",
    "from owlready2 import get_ontology, default_world\n",
    "import xmltodict\n",
    "import time\n",
    "from rdflib import URIRef\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "# one liner to import cogtext package from `../python` folder.\n",
    "if '../python' not in sys.path: sys.path.append('../python'); from cogtext import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl_file = \"../data/ontologies/efo.owl\"\n",
    "owl_prefix = \"http://behaverse.github.io/ontologies/2019/11/executive-functions-ontology#\"\n",
    "\n",
    "ONTOLOGY = get_ontology(owl_file).load()\n",
    "GRAPH = default_world.as_rdflib_graph()\n",
    "\n",
    "def query(graph, parent_cls='Task'):\n",
    "    \"\"\"Function to query tasks, constructs, regions, etc.\n",
    "\n",
    "    ## Returns\n",
    "    A list of labels\n",
    "    \"\"\"\n",
    "\n",
    "    cls_name = parent_cls[1:] if parent_cls.startswith(\":\") else parent_cls\n",
    "\n",
    "    sparql_query = f\"\"\"\n",
    "    prefix : <{owl_prefix}>\n",
    "\n",
    "    SELECT ?label ?pubmed_query\n",
    "    WHERE {{\n",
    "    ?cls rdfs:subClassOf* :{cls_name};\n",
    "          :pubmedQuery ?pubmed_query;\n",
    "          rdfs:label ?label\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    print(sparql_query)\n",
    "    # select the all rdfs:labels, flatten the list of labels, and convert them to python string\n",
    "    labels = [labels for labels in graph.query(sparql_query)]\n",
    "    pubmed_queries = {l[0].toPython(): l[1].toPython() for l in labels}\n",
    "    return pubmed_queries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n    prefix : <http://behaverse.github.io/ontologies/2019/11/executive-functions-ontology#>\n\n    SELECT ?label ?pubmed_query\n    WHERE {\n    ?task rdfs:subClassOf* :CognitiveProcess;\n          :pubmedQuery ?pubmed_query;\n          rdfs:label ?label\n    }\n    \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# pubmed_queries = query(GRAPH, 'BaggettaTask')   # dictionary of task_name -> pubmed_query\n",
    "# print(f'{len(pubmed_queries)} tasks found in the EF ontology (baggetta2016)')\n",
    "\n",
    "pubmed_queries = query(GRAPH, 'CognitiveProcess')   # dictionary of task_name -> pubmed_query\n",
    "pubmed_queries\n"
   ]
  },
  {
   "source": [
    "# search and cache hits\n",
    "\n",
    "Let's search PubMed for each task query (`ef:pubmedQuery`), then cleanup the XML results into a CSV with the following columns: abstract, title, and a reference to the metadata (pmid).\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[PubMed] (Degraded Vigilance task* \"PVT\"[TIAB])\n",
      "[PubMed] Succesfully stored 4 hits on NCBI history server.\n",
      "[PubMed] Succesfully stored hits in ../data/pubmed/.cache/Degraded Vigilance.xml.\n",
      "[PubMed] (\"Delay Choice\" Task*[TIAB])\n",
      "[PubMed] Succesfully stored 15 hits on NCBI history server.\n",
      "[PubMed] Succesfully stored hits in ../data/pubmed/.cache/Delay Choice.xml.\n",
      "[PubMed] (\"Dimensional Change Card\" Sort*[TIAB])\n",
      "[PubMed] Succesfully stored 134 hits on NCBI history server.\n",
      "[PubMed] Succesfully stored hits in ../data/pubmed/.cache/DCCS (Dimentional Change Card Sort).xml.\n",
      "[XML2CSV] converting \"Degraded Vigilance\" dataset...\n",
      "[XML2CSV] converting \"Delay Choice\" dataset...\n",
      "[XML2CSV] converting \"DCCS (Dimentional Change Card Sort)\" dataset...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "for task_label, pubmed_query in pubmed_queries.items():\n",
    "    task_label = task_label.replace('/','') \n",
    "    fname = Path('../data/pubmed/.cache') / (task_label + '.xml')\n",
    "\n",
    "    if not fname.exists():\n",
    "        search_and_store(pubmed_query, fname)\n",
    "\n",
    "\n",
    "def find_mesh(mesh_list):\n",
    "    \"\"\"Extracts MeSH names from a list of XML MedlineCitation.MeshHeadingList.MeshHeading tags.\"\"\"\n",
    "    if not isinstance(mesh_list, list):\n",
    "        return []\n",
    "\n",
    "    mesh_names = [h['DescriptorName']['#text'] for h in mesh_list]# if d['DescriptorName']['@MajorTopicYN'] == 'Y']\n",
    "    return mesh_names\n",
    "\n",
    "\n",
    "# now cleanup and convert all abstracts into CSV files\n",
    "for task_label in pubmed_queries.keys():\n",
    "\n",
    "    task_fname = task_label.replace('/','')\n",
    "\n",
    "    xml_file = Path('../data/pubmed/.cache') / (task_fname + '.xml')\n",
    "    csv_file = Path('../data/pubmed/tests') / (task_fname + '.csv')\n",
    "\n",
    "    if not csv_file.exists():\n",
    "        with open(xml_file, 'r') as f:\n",
    "            xml_content = xmltodict.parse(f.read())\n",
    "            if 'PubmedArticleSet' in xml_content:\n",
    "                print(f'[XML2CSV] converting \"{task_label}\" dataset...')\n",
    "\n",
    "                df = pd.json_normalize(xml_content['PubmedArticleSet']['PubmedArticle'])\n",
    "\n",
    "                # pmid, title, and abstract\n",
    "                df['pmid'] = df['MedlineCitation.PMID.#text']\n",
    "                df['title'] = df['MedlineCitation.Article.ArticleTitle']\n",
    "                df['abstract'] = df['MedlineCitation.Article.Abstract.AbstractText'].apply(cleanup_abstract)\n",
    "                \n",
    "                # publication year\n",
    "                df['year'] = df['MedlineCitation.Article.Journal.JournalIssue.PubDate.Year']\n",
    "                df['journal_title'] = df['MedlineCitation.Article.Journal.Title']\n",
    "                df['journal_iso_abbreviation'] = df['MedlineCitation.Article.Journal.ISOAbbreviation']\n",
    "\n",
    "                # MeSh topics (some datasets do not contain MeshHeading, e.g., Spin The Pots)\n",
    "                # if 'MedlineCitation.MeshHeadingList.MeshHeading' in df.columns:\n",
    "                #     df['mesh'] = df['MedlineCitation.MeshHeadingList.MeshHeading'].apply(find_mesh)\n",
    "                # else:\n",
    "                #     df['mesh'] = None\n",
    "\n",
    "                if 'MedlineCitation.Article.Journal.JournalIssue.PubDate.MedlineDate' in df.columns:\n",
    "                    medline_year = df['MedlineCitation.Article.Journal.JournalIssue.PubDate.MedlineDate'].apply(lambda x: x[0:4] if isinstance(x, str) and len(x)>=4 else x)\n",
    "                    df['year'].fillna(medline_year, inplace=True)\n",
    "\n",
    "                # fill missing abstracts with #text value\n",
    "                if 'MedlineCitation.Article.Abstract.AbstractText.#text' in df.columns:\n",
    "                    df['abstract'].fillna(df['MedlineCitation.Article.Abstract.AbstractText.#text'], inplace=True)\n",
    "\n",
    "                if 'MedlineCitation.Article.ArticleTitle.#text' in df.columns:\n",
    "                    df['title'].fillna(df['MedlineCitation.Article.ArticleTitle.#text'], inplace=True)\n",
    "\n",
    "                df[['pmid', 'year', 'journal_title', 'journal_iso_abbreviation', 'title','abstract']].to_csv(csv_file, index=False)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "source": [
    "## Data cleansing\n",
    "\n",
    "Here, we aim to preprocess PubMed corpora and keep only those relevant metadata. Outputs of this pipeline are stored in the `data/pubmed` folder as csv files; one csv per task corpus.\n",
    "\n",
    "The following metadata will be stored in the processed csv files:\n",
    "\n",
    "- pmid: unique PubMed identifier of the article.\n",
    "- title: escaped title in string format.\n",
    "- journal_title: Journal title\n",
    "- journal_iso_abbreviation: Journal ISO abbreviation\n",
    "- abstract: escaped and cleanedup abstract in string format.\n",
    "- year: publication year in YYYY format.\n",
    "- mesh: A list of Medical Subject Headings which contains the field of research and other topics. We only keep major topics."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}