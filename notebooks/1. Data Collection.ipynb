{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "ea20dcc4936bc3e336221d5e5ae7bcd09719da9b70640ea666426cf2d919dc3d"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "* Owlready2 * Warning: optimized Cython parser module 'owlready2_optimized' is not available, defaulting to slower Python implementation\n"
     ]
    }
   ],
   "source": [
    "from rdflib import OWL, Graph\n",
    "from rdflib.namespace import RDFS\n",
    "from owlready2 import get_ontology, default_world\n",
    "import xmltodict\n",
    "from collections import OrderedDict \n",
    "import time\n",
    "from rdflib import URIRef\n",
    "import random\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "\n",
    "# one liner to import cogtext package from `../python` folder.\n",
    "if '../python' not in sys.path: sys.path.append('../python'); from cogtext import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "owl_file = \"../data/ontologies/efo.owl\"\n",
    "owl_prefix = \"http://www.semanticweb.org/morteza/ontologies/2019/11/executive-functions-ontology#\"\n",
    "\n",
    "ONTOLOGY = get_ontology(owl_file).load()\n",
    "GRAPH = default_world.as_rdflib_graph()\n",
    "\n",
    "def query(graph, parent_cls='Task'):\n",
    "    \"\"\"Function to query tasks, constructs, regions, etc.\n",
    "\n",
    "    ## Returns\n",
    "    A list of labels\n",
    "    \"\"\"\n",
    "\n",
    "    cls_name = parent_cls[1:] if parent_cls.startswith(\":\") else parent_cls\n",
    "\n",
    "    query = f\"\"\"\n",
    "    prefix : <{owl_prefix}>\n",
    "\n",
    "    SELECT ?label\n",
    "    WHERE {{\n",
    "    ?task rdfs:subClassOf* :{cls_name};\n",
    "            rdfs:label ?label\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    # select the all rdfs:labels, flatten the list of labels, and convert them to python string\n",
    "    labels = [labels for labels in graph.query(query)]\n",
    "    flatten_labels = [l.toPython() for ll in labels for l in ll]\n",
    "    return flatten_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "134 tasks found in the ontology (enkavi, baggetta, dimond, etc).\n"
     ]
    }
   ],
   "source": [
    "task_terms = query(GRAPH, 'CognitiveTask')\n",
    "\n",
    "print(f'{len(task_terms)} tasks found in the ontology (enkavi, baggetta, dimond, etc).')"
   ]
  },
  {
   "source": [
    "The following cell uses a custom-made PubMed client to search, cache and fetch results. This client uses NCBI history API to make searching scalable, so it is not limitation on the number of results."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pybliometrics.scopus import ScopusSearch\n",
    "# s = ScopusSearch('Digit Span')\n",
    "# print(s)\n",
    "# fetch = PubMedFetcher(cachedir='../data/cache/pubmed/')"
   ]
  },
  {
   "source": [
    "# search and cache results!\n",
    "i\n",
    "for term in task_terms:\n",
    "    term = term.lower()\n",
    "    # to avoid '/' in terms turning into path separator\n",
    "    fpath = Path('../data/cache') / (term.replace('/','') + '.xml')\n",
    "    if not fpath.exists():\n",
    "        t0 = \n",
    "        search_and_store(term.lower(), fpath, suffix=['task', 'test'])"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Now, let's cleanup the XML mess and keep only abstract, title, and some references to the origianl data."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "converting \"Cognitive Control\" dataset...\n",
      "converting \"Executive Function\" dataset...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def cleanup_abstract(abstract_text):\n",
    "    \"\"\"PubMed returns abstract with semantic tags. This function cleans those tags and keep the text.\"\"\"\n",
    "    select_content = lambda c: c if isinstance(c ,str) else c['#text'] if (c is not None and ('#text' in c)) else ''\n",
    "\n",
    "    if isinstance(abstract_text, list):\n",
    "        return ' '.join([select_content(a) for a in abstract_text])\n",
    "    elif isinstance(abstract_text, OrderedDict):\n",
    "        return select_content(abstract_text)\n",
    "    return abstract_text    # when the abstract is string\n",
    "\n",
    "\n",
    "# now cleanup and convert all abstracts into CSV files\n",
    "for term in task_terms:\n",
    "    print(f'converting \"{term}\" dataset...')\n",
    "\n",
    "    in_path = Path('../data/cache') / (term.replace('/','') + '.xml')\n",
    "    out_path = Path('../data/pubmed') / (term.replace('/','') + '.csv')\n",
    "\n",
    "    with open(in_path, 'r') as f:\n",
    "        cache = xmltodict.parse(f.read())\n",
    "        if 'PubmedArticleSet' in cache:\n",
    "\n",
    "            df = pd.json_normalize(cache['PubmedArticleSet']['PubmedArticle'])\n",
    "\n",
    "            # df[df['MedlineCitation.PMID.#text'] == \"33686858\"]['MedlineCitation.Article.Abstract.AbstractText']\n",
    "            df['abstract'] = df['MedlineCitation.Article.Abstract.AbstractText'].apply(cleanup_abstract)\n",
    "            df['pmid'] = df['MedlineCitation.PMID.#text']\n",
    "            df['title'] = df['MedlineCitation.Article.ArticleTitle']\n",
    "\n",
    "            # fill missing abstracts with #text\n",
    "            if 'MedlineCitation.Article.Abstract.AbstractText.#text' in df.columns:\n",
    "                df['abstract'].fillna(df['MedlineCitation.Article.Abstract.AbstractText.#text'], inplace=True)\n",
    "\n",
    "            if 'MedlineCitation.Article.ArticleTitle.#text' in df.columns:\n",
    "                df['title'].fillna(df['MedlineCitation.Article.ArticleTitle.#text'], inplace=True)\n",
    "\n",
    "            df[['title','abstract','pmid']].to_csv(out_path, index=False)\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  }
 ]
}